{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Org the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on macOS 15.4\n",
      "MPS is built: True\n",
      "MPS is available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# Check if running on macOS\n",
    "if platform.system() == 'Darwin':\n",
    "    # Check for MPS (Metal Performance Shaders) availability on Mac\n",
    "    print(f\"Running on macOS {platform.mac_ver()[0]}\")\n",
    "    print(f\"MPS is built: {torch.backends.mps.is_built()}\")\n",
    "    print(f\"MPS is available: {torch.backends.mps.is_available()}\")\n",
    "else:\n",
    "    print(f\"Running on {platform.system()} {platform.release()}\")\n",
    "    if platform.system() == 'Windows':\n",
    "        # Check for CUDA availability on Windows\n",
    "        print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA version: {torch.version.cuda}\")\n",
    "            print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "            print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "            print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"MPS is only available on macOS devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def extract_action_samples(csv_file, plot_stats=False):\n",
    "    \"\"\"\n",
    "    Extract action samples from a CSV file according to specific rules.\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Full path to the CSV file\n",
    "        plot_stats (bool): If True, plot statistics about the extracted samples\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed data with action groups and group indices\n",
    "        list: Group start times\n",
    "        datetime: Start time of the whole process (timestamp of first record)\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Ensure timestamp is in datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Get the start time of the whole process (first record)\n",
    "    process_start_time = df['timestamp'].iloc[0] if not df.empty else None\n",
    "    \n",
    "    # Initialize variables\n",
    "    action_groups = []\n",
    "    group_start_times = []\n",
    "    current_idx = 0\n",
    "    all_extracted_records = []\n",
    "    \n",
    "    # Iterate through dataframe to find action groups\n",
    "    while current_idx < len(df) - 60:  # Need at least 60 records for a complete group\n",
    "        # Look for a cross button press\n",
    "        if df.loc[current_idx + 10, 'button_press'] == 'cross':\n",
    "            # Check if we have 10 'none' records before the press\n",
    "            pre_press = df.iloc[current_idx:current_idx + 10]\n",
    "            if all(pre_press['button_press'] == 'none'):\n",
    "                # Extract the group: 10 records before press + 50 records starting from press\n",
    "                action_group = df.iloc[current_idx:current_idx + 60]\n",
    "                \n",
    "                # Calculate duration\n",
    "                start_time = action_group.iloc[0]['timestamp']\n",
    "                end_time = action_group.iloc[-1]['timestamp']\n",
    "                duration = (end_time - start_time).total_seconds()\n",
    "                \n",
    "                # Store group and start time\n",
    "                action_groups.append({\n",
    "                    'group': action_group,\n",
    "                    'duration': duration\n",
    "                })\n",
    "                group_start_times.append(start_time)\n",
    "                \n",
    "                # Add group_index to records\n",
    "                group_index = len(action_groups) - 1\n",
    "                group_with_index = action_group.copy()\n",
    "                group_with_index['group_index'] = group_index\n",
    "                \n",
    "                # Add to collection\n",
    "                all_extracted_records.append(group_with_index)\n",
    "                \n",
    "                # Move to position after this group\n",
    "                current_idx += 60\n",
    "            else:\n",
    "                # Pre-press condition not met, move forward by 1\n",
    "                current_idx += 1\n",
    "        else:\n",
    "            # No press at expected position, move forward by 1\n",
    "            current_idx += 1\n",
    "    \n",
    "    # Combine all extracted records\n",
    "    if all_extracted_records:\n",
    "        extracted_df = pd.concat(all_extracted_records)\n",
    "        print(f\"Extracted {len(action_groups)} action groups from {csv_file}\")\n",
    "        \n",
    "        # Print the shape of the sample and count of how many samples\n",
    "        print(f\"Shape of extracted data: {extracted_df.shape}\")\n",
    "        print(f\"Total number of samples: {len(extracted_df)}\")\n",
    "        print(f\"Number of unique groups: {extracted_df['group_index'].nunique()}\")\n",
    "        \n",
    "        # Print the shape of each individual sample (group)\n",
    "        print(\"\\nShape of each sample (group):\")\n",
    "        for i, group in enumerate(action_groups):\n",
    "            group_df = group['group']\n",
    "            print(f\"Group {i}: {group_df.shape} - Duration: {group['duration']:.2f}s\")\n",
    "        \n",
    "        # If plot_stats is True, visualize detailed statistics about the samples\n",
    "        if plot_stats:\n",
    "            # Create a figure with multiple subplots\n",
    "            fig, axes = plt.subplots(len(action_groups), 3, figsize=(18, 4*len(action_groups)))\n",
    "            \n",
    "            # If only one group, make axes indexable\n",
    "            if len(action_groups) == 1:\n",
    "                axes = np.array([axes])\n",
    "                \n",
    "            for i, group in enumerate(action_groups):\n",
    "                group_df = group['group']\n",
    "                \n",
    "                # Count statistics\n",
    "                pre_press_count = sum(group_df.iloc[:10]['button_press'] == 'none')\n",
    "                press_events = group_df.iloc[10:]['button_press'].value_counts()\n",
    "                press_count = sum(group_df.iloc[10:]['button_press'] != 'none')\n",
    "                post_press_none_count = sum(group_df.iloc[10:]['button_press'] == 'none')\n",
    "                \n",
    "                # Plot 1: Button press distribution\n",
    "                press_events_df = pd.DataFrame(press_events).reset_index()\n",
    "                press_events_df.columns = ['Button', 'Count']\n",
    "                sns.barplot(x='Button', y='Count', data=press_events_df, ax=axes[i, 0])\n",
    "                axes[i, 0].set_title(f'Group {i}: Button Press Distribution')\n",
    "                axes[i, 0].set_ylabel('Count')\n",
    "                axes[i, 0].tick_params(axis='x', rotation=45)\n",
    "                \n",
    "                # Plot 2: Gyro data over time\n",
    "                time_indices = range(len(group_df))\n",
    "                axes[i, 1].plot(time_indices, group_df['gyro_pitch'], label='Pitch')\n",
    "                axes[i, 1].plot(time_indices, group_df['gyro_yaw'], label='Yaw')\n",
    "                axes[i, 1].plot(time_indices, group_df['gyro_roll'], label='Roll')\n",
    "                axes[i, 1].axvline(x=10, color='r', linestyle='--', label='First Press')\n",
    "                axes[i, 1].set_title(f'Group {i}: Gyro Data')\n",
    "                axes[i, 1].set_xlabel('Time Index')\n",
    "                axes[i, 1].set_ylabel('Gyro Values')\n",
    "                axes[i, 1].legend()\n",
    "                \n",
    "                # Plot 3: Accelerometer data over time\n",
    "                axes[i, 2].plot(time_indices, group_df['acc_x'], label='X')\n",
    "                axes[i, 2].plot(time_indices, group_df['acc_y'], label='Y')\n",
    "                axes[i, 2].plot(time_indices, group_df['acc_z'], label='Z')\n",
    "                axes[i, 2].axvline(x=10, color='r', linestyle='--', label='First Press')\n",
    "                axes[i, 2].set_title(f'Group {i}: Accelerometer Data')\n",
    "                axes[i, 2].set_xlabel('Time Index')\n",
    "                axes[i, 2].set_ylabel('Accel Values')\n",
    "                axes[i, 2].legend()\n",
    "                \n",
    "                # Add text annotation with statistics\n",
    "                stats_text = (f\"Duration: {group['duration']:.2f}s\\n\"\n",
    "                             f\"Pre-press none: {pre_press_count}\\n\"\n",
    "                             f\"Button presses: {press_count}\\n\"\n",
    "                             f\"Post-press none: {post_press_none_count}\")\n",
    "                axes[i, 0].annotate(stats_text, xy=(0.5, -0.4), xycoords='axes fraction', \n",
    "                                   ha='center', va='center', fontsize=10,\n",
    "                                   bbox=dict(boxstyle='round', fc='lightyellow', alpha=0.7))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Create a summary plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Plot 1: Group durations\n",
    "            plt.subplot(2, 2, 1)\n",
    "            durations = [group['duration'] for group in action_groups]\n",
    "            plt.bar(range(len(durations)), durations)\n",
    "            plt.xlabel('Group Index')\n",
    "            plt.ylabel('Duration (seconds)')\n",
    "            plt.title('Duration of Each Action Group')\n",
    "            \n",
    "            # Plot 2: Button press distribution across all groups\n",
    "            plt.subplot(2, 2, 2)\n",
    "            sns.countplot(x='button_press', data=extracted_df)\n",
    "            plt.title('Button Press Distribution')\n",
    "            plt.xlabel('Button Type')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "            # Plot 3: Gyro data distribution\n",
    "            plt.subplot(2, 2, 3)\n",
    "            sns.boxplot(data=extracted_df[['gyro_pitch', 'gyro_yaw', 'gyro_roll']])\n",
    "            plt.title('Gyro Data Distribution')\n",
    "            plt.ylabel('Values')\n",
    "            \n",
    "            # Plot 4: Accelerometer data distribution\n",
    "            plt.subplot(2, 2, 4)\n",
    "            sns.boxplot(data=extracted_df[['acc_x', 'acc_y', 'acc_z']])\n",
    "            plt.title('Accelerometer Data Distribution')\n",
    "            plt.ylabel('Values')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Print basic information about the extracted data\n",
    "        print(\"\\nExtracted Data Overview:\")\n",
    "        print(f\"Total records: {len(extracted_df)}\")\n",
    "        print(f\"Number of groups: {len(action_groups)}\")\n",
    "        print(f\"Columns: {extracted_df.columns.tolist()}\")\n",
    "        print(\"\\nSample data (first 5 rows):\")\n",
    "        print(extracted_df.head())\n",
    "        print(\"\\nButton press distribution:\")\n",
    "        print(extracted_df['button_press'].value_counts())\n",
    "            \n",
    "        return extracted_df, group_start_times, process_start_time\n",
    "    else:\n",
    "        print(f\"No action groups found in {csv_file}\")\n",
    "        return pd.DataFrame(), [], process_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import os\n",
    "\n",
    "def split_audio_by_timestamps(wav_file, group_start_times, process_start_time, segment_duration=0.3, \n",
    "                            half_peak_duration=0.03, plot_spectrograms=False):\n",
    "    \"\"\"\n",
    "    Split audio file into segments based on group start times from action samples.\n",
    "    \n",
    "    Args:\n",
    "        wav_file (str): Path to the WAV file\n",
    "        group_start_times (list): List of timestamps for each group start\n",
    "        process_start_time (datetime): Start time of the whole process\n",
    "        segment_duration (float): Duration of each segment in seconds (default: 0.3)\n",
    "        half_peak_duration (float): Half duration of peak window in seconds (default: 0.03)\n",
    "        plot_spectrograms (bool): Whether to plot spectrograms (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains processed audio data with following keys:\n",
    "            - 'audio_segments': List of raw audio segments\n",
    "            - 'peak_segments': List of peak audio segments\n",
    "            - 'spectrograms': List of spectrogram data\n",
    "            - 'mfccs': List of MFCC features\n",
    "            - 'low_power_peaks': List of indices of low power peak segments\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio, sr = sf.read(wav_file)\n",
    "    \n",
    "    # Initialize storage for results\n",
    "    audio_segments = []\n",
    "    peak_segments = []\n",
    "    spectrograms = []\n",
    "    mfccs = []\n",
    "    low_power_peaks = []\n",
    "    off_center_peaks = []\n",
    "    \n",
    "    # Process each group start time\n",
    "    for i, start_time in enumerate(group_start_times):\n",
    "        # Calculate the start index in audio samples\n",
    "        relative_seconds = (start_time - process_start_time).total_seconds()\n",
    "        start_idx = int(relative_seconds * sr)\n",
    "        end_idx = start_idx + int(segment_duration * sr)\n",
    "        \n",
    "        # Ensure indices are within bounds\n",
    "        if start_idx >= 0 and end_idx <= len(audio):\n",
    "            # Extract the segment\n",
    "            segment = audio[start_idx:end_idx]\n",
    "            audio_segments.append(segment)\n",
    "            \n",
    "            # Convert to mono if stereo\n",
    "            if len(segment.shape) > 1 and segment.shape[1] > 1:\n",
    "                segment_mono = np.mean(segment, axis=1)\n",
    "            else:\n",
    "                segment_mono = segment\n",
    "            \n",
    "            # Apply Wiener filter for denoising\n",
    "            noise_samples = int(0.03 * sr)\n",
    "            noise = segment_mono[:noise_samples]\n",
    "            noise_psd = 0.0005 * np.mean(np.abs(np.fft.rfft(noise))**2)\n",
    "            denoised_segment = signal.wiener(segment_mono, mysize=1024, noise=noise_psd)\n",
    "            \n",
    "            # Find peak in the middle section\n",
    "            start_exclude_idx = int(0.01 * sr)\n",
    "            end_exclude_idx = len(denoised_segment) - int(0.01 * sr)\n",
    "            valid_segment = denoised_segment[start_exclude_idx:end_exclude_idx]\n",
    "            \n",
    "            # Peak detection\n",
    "            abs_segment = np.abs(valid_segment)\n",
    "            max_amplitude = np.max(abs_segment)\n",
    "            peaks, peak_properties = signal.find_peaks(abs_segment,\n",
    "                                                     prominence=0.2*max_amplitude,  # Lowered prominence threshold\n",
    "                                                     distance=int(0.01*sr))\n",
    "            \n",
    "            # If no peaks found, use maximum value\n",
    "            if len(peaks) == 0:\n",
    "                peak_index = np.argmax(abs_segment) + start_exclude_idx\n",
    "                peak_properties = {'prominences': [0]}\n",
    "            else:\n",
    "                highest_peak_idx = np.argmax(peak_properties['prominences'])\n",
    "                peak_index = peaks[highest_peak_idx] + start_exclude_idx\n",
    "            \n",
    "            peak_time = peak_index / sr\n",
    "            \n",
    "            # Calculate peak window\n",
    "            peak_start_time = max(0.0, peak_time - half_peak_duration)\n",
    "            peak_end_time = min(segment_duration, peak_time + half_peak_duration)\n",
    "            peak_start_idx = int(peak_start_time * sr)\n",
    "            peak_end_idx = int(peak_end_time * sr)\n",
    "            \n",
    "            # Extract peak window\n",
    "            peak_segment = denoised_segment[peak_start_idx:peak_end_idx]\n",
    "            \n",
    "            # New low power peak detection logic\n",
    "            peak_absolute_value = np.abs(denoised_segment[peak_index])\n",
    "            is_low_power = peak_absolute_value < 0.05  # Mark as low power if absolute peak value < 0.05\n",
    "            \n",
    "            if is_low_power:\n",
    "                low_power_peaks.append(i)\n",
    "            \n",
    "            # Check if peak is centered\n",
    "            window_duration = peak_end_time - peak_start_time\n",
    "            middle_time = peak_start_time + window_duration / 2\n",
    "            tolerance = window_duration * 0.1\n",
    "            is_off_center = abs(peak_time - middle_time) > tolerance\n",
    "            if is_off_center:\n",
    "                off_center_peaks.append(i)\n",
    "            \n",
    "            # Compute spectrogram\n",
    "            n_fft = 256\n",
    "            hop_length = 128\n",
    "            frequencies, times, Sxx = signal.spectrogram(peak_segment, sr,\n",
    "                                                       nperseg=n_fft,\n",
    "                                                       noverlap=n_fft-hop_length,\n",
    "                                                       scaling='density')\n",
    "            \n",
    "            Sxx_db = 10 * np.log10(Sxx + 1e-10)\n",
    "            \n",
    "            # Only save the spectrogram data\n",
    "            spectrograms.append(Sxx_db)\n",
    "            \n",
    "            # Compute MFCC\n",
    "            stft = librosa.stft(peak_segment, n_fft=n_fft, hop_length=hop_length)\n",
    "            mel_spec = librosa.feature.melspectrogram(S=np.abs(stft)**2, sr=sr, n_mels=40)\n",
    "            mfcc_features = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec), \n",
    "                                               n_mfcc=13, fmax=12000, fmin=0)\n",
    "            \n",
    "            # Only save the MFCC data\n",
    "            mfccs.append(mfcc_features)\n",
    "            \n",
    "            peak_segments.append(peak_segment)\n",
    "    \n",
    "    # Print statistics with more detail\n",
    "    print(f\"Processed {len(audio_segments)} segments\")\n",
    "    print(f\"Groups with off-center peaks: {off_center_peaks}\")\n",
    "    print(f\"Number of low power peaks (abs peak < 0.05): {len(low_power_peaks)}\")\n",
    "    print(f\"Low power peak group indices: {low_power_peaks}\")\n",
    "    \n",
    "    # Calculate percentage of low power peaks\n",
    "    low_power_percentage = (len(low_power_peaks) / len(audio_segments)) * 100 if audio_segments else 0\n",
    "    print(f\"Percentage of low power peaks: {low_power_percentage:.2f}%\")\n",
    "    \n",
    "    peak_values = [np.max(np.abs(peak_seg)) for peak_seg in peak_segments]\n",
    "    print(f\"\\nPeak amplitude statistics:\")\n",
    "    print(f\"Min peak amplitude: {min(peak_values):.3f}\")\n",
    "    print(f\"Max peak amplitude: {max(peak_values):.3f}\")\n",
    "    print(f\"Mean peak amplitude: {np.mean(peak_values):.3f}\")\n",
    "    \n",
    "    # Print sample shapes\n",
    "    if spectrograms and mfccs:\n",
    "        print(f\"\\nSample spectrogram shape frequency bins , number of time slices: {spectrograms[0].shape[0]} , {spectrograms[0].shape[1 ]}\")\n",
    "        print(f\"Sample MFCC shape number of mfcc coefficients , number of time slices: {mfccs[0].shape[0]} , {mfccs[0].shape[1 ]}\")\n",
    "        print('\\n')\n",
    "    \n",
    "    if plot_spectrograms and spectrograms:\n",
    "        # Plot configuration\n",
    "        num_specs = len(spectrograms)\n",
    "        cols = 10\n",
    "        rows = (num_specs + cols - 1) // cols\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 2 * rows))\n",
    "        gs = GridSpec(rows, cols, figure=fig)\n",
    "        \n",
    "        for i in range(len(spectrograms)):\n",
    "            row = i // cols\n",
    "            col = i % cols\n",
    "            \n",
    "            ax = fig.add_subplot(gs[row, col])\n",
    "            denoised_segment_to_plot = audio_segments[i]\n",
    "            if len(denoised_segment_to_plot.shape) > 1:\n",
    "                denoised_segment_to_plot = np.mean(denoised_segment_to_plot, axis=1)\n",
    "            \n",
    "            time_axis = np.linspace(0, segment_duration, len(denoised_segment_to_plot))\n",
    "            ax.plot(time_axis, denoised_segment_to_plot)\n",
    "            \n",
    "            # Simplified title\n",
    "            title_color = 'red' if i in low_power_peaks else 'black'\n",
    "            ax.set_title(f'Group {i}', color=title_color)\n",
    "            \n",
    "            if row < rows - 1:\n",
    "                ax.set_xlabel('')\n",
    "            if col > 0:\n",
    "                ax.set_ylabel('')\n",
    "            else:\n",
    "                ax.set_ylabel('Amplitude')\n",
    "        \n",
    "        plt.suptitle('Denoised audio segments (Y-axis: normalized amplitude [-1, 1])', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'audio_segments': audio_segments,\n",
    "        'peak_segments': peak_segments,\n",
    "        'spectrograms': spectrograms,\n",
    "        'mfccs': mfccs,\n",
    "        'low_power_peaks': low_power_peaks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read the data \n",
    "\n",
    "user1_offh_csv=\"../data/fuse_one/controller_data_user1_non-haptic_20250328_181856.csv\"\n",
    "user2_offh_csv=\"../data/fuse_one/controller_data_user2_non-haptic_20250327_152659.csv\"\n",
    "user3_offh_csv='../data/fuse_one/controller_data_user3_non-haptic_20250327_160855.csv'\n",
    "user4_offh_csv='../data/fuse_one/controller_data_user4_non-haptic_20250327_165510.csv'\n",
    "\n",
    "user1_offh_wav=\"../data/fuse_one/audio_user1_non-haptic_20250328_181856.wav\"\n",
    "user2_offh_wav=\"../data/fuse_one/audio_user2_non-haptic_20250327_152659.wav\"\n",
    "user3_offh_wav='../data/fuse_one/audio_user3_non-haptic_20250327_160855.wav'\n",
    "user4_offh_wav='../data/fuse_one/audio_user4_non-haptic_20250327_165510.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 126 action groups from ../data/fuse_one/controller_data_user1_non-haptic_20250328_181856.csv\n",
      "Shape of extracted data: (7560, 11)\n",
      "Total number of samples: 7560\n",
      "Number of unique groups: 126\n",
      "\n",
      "Shape of each sample (group):\n",
      "Group 0: (60, 10) - Duration: 0.29s\n",
      "Group 1: (60, 10) - Duration: 0.30s\n",
      "Group 2: (60, 10) - Duration: 0.30s\n",
      "Group 3: (60, 10) - Duration: 0.29s\n",
      "Group 4: (60, 10) - Duration: 0.30s\n",
      "Group 5: (60, 10) - Duration: 0.30s\n",
      "Group 6: (60, 10) - Duration: 0.30s\n",
      "Group 7: (60, 10) - Duration: 0.30s\n",
      "Group 8: (60, 10) - Duration: 0.29s\n",
      "Group 9: (60, 10) - Duration: 0.30s\n",
      "Group 10: (60, 10) - Duration: 0.30s\n",
      "Group 11: (60, 10) - Duration: 0.30s\n",
      "Group 12: (60, 10) - Duration: 0.30s\n",
      "Group 13: (60, 10) - Duration: 0.30s\n",
      "Group 14: (60, 10) - Duration: 0.30s\n",
      "Group 15: (60, 10) - Duration: 0.29s\n",
      "Group 16: (60, 10) - Duration: 0.30s\n",
      "Group 17: (60, 10) - Duration: 0.29s\n",
      "Group 18: (60, 10) - Duration: 0.29s\n",
      "Group 19: (60, 10) - Duration: 0.29s\n",
      "Group 20: (60, 10) - Duration: 0.30s\n",
      "Group 21: (60, 10) - Duration: 0.30s\n",
      "Group 22: (60, 10) - Duration: 0.30s\n",
      "Group 23: (60, 10) - Duration: 0.29s\n",
      "Group 24: (60, 10) - Duration: 0.29s\n",
      "Group 25: (60, 10) - Duration: 0.29s\n",
      "Group 26: (60, 10) - Duration: 0.30s\n",
      "Group 27: (60, 10) - Duration: 0.30s\n",
      "Group 28: (60, 10) - Duration: 0.30s\n",
      "Group 29: (60, 10) - Duration: 0.30s\n",
      "Group 30: (60, 10) - Duration: 0.29s\n",
      "Group 31: (60, 10) - Duration: 0.29s\n",
      "Group 32: (60, 10) - Duration: 0.30s\n",
      "Group 33: (60, 10) - Duration: 0.30s\n",
      "Group 34: (60, 10) - Duration: 0.29s\n",
      "Group 35: (60, 10) - Duration: 0.30s\n",
      "Group 36: (60, 10) - Duration: 0.29s\n",
      "Group 37: (60, 10) - Duration: 0.30s\n",
      "Group 38: (60, 10) - Duration: 0.29s\n",
      "Group 39: (60, 10) - Duration: 0.30s\n",
      "Group 40: (60, 10) - Duration: 0.29s\n",
      "Group 41: (60, 10) - Duration: 0.30s\n",
      "Group 42: (60, 10) - Duration: 0.29s\n",
      "Group 43: (60, 10) - Duration: 0.30s\n",
      "Group 44: (60, 10) - Duration: 0.29s\n",
      "Group 45: (60, 10) - Duration: 0.29s\n",
      "Group 46: (60, 10) - Duration: 0.30s\n",
      "Group 47: (60, 10) - Duration: 0.29s\n",
      "Group 48: (60, 10) - Duration: 0.29s\n",
      "Group 49: (60, 10) - Duration: 0.29s\n",
      "Group 50: (60, 10) - Duration: 0.29s\n",
      "Group 51: (60, 10) - Duration: 0.29s\n",
      "Group 52: (60, 10) - Duration: 0.29s\n",
      "Group 53: (60, 10) - Duration: 0.30s\n",
      "Group 54: (60, 10) - Duration: 0.29s\n",
      "Group 55: (60, 10) - Duration: 0.30s\n",
      "Group 56: (60, 10) - Duration: 0.29s\n",
      "Group 57: (60, 10) - Duration: 0.29s\n",
      "Group 58: (60, 10) - Duration: 0.29s\n",
      "Group 59: (60, 10) - Duration: 0.30s\n",
      "Group 60: (60, 10) - Duration: 0.29s\n",
      "Group 61: (60, 10) - Duration: 0.29s\n",
      "Group 62: (60, 10) - Duration: 0.29s\n",
      "Group 63: (60, 10) - Duration: 0.30s\n",
      "Group 64: (60, 10) - Duration: 0.30s\n",
      "Group 65: (60, 10) - Duration: 0.29s\n",
      "Group 66: (60, 10) - Duration: 0.29s\n",
      "Group 67: (60, 10) - Duration: 0.30s\n",
      "Group 68: (60, 10) - Duration: 0.29s\n",
      "Group 69: (60, 10) - Duration: 0.30s\n",
      "Group 70: (60, 10) - Duration: 0.34s\n",
      "Group 71: (60, 10) - Duration: 0.30s\n",
      "Group 72: (60, 10) - Duration: 0.30s\n",
      "Group 73: (60, 10) - Duration: 0.29s\n",
      "Group 74: (60, 10) - Duration: 0.29s\n",
      "Group 75: (60, 10) - Duration: 0.30s\n",
      "Group 76: (60, 10) - Duration: 0.29s\n",
      "Group 77: (60, 10) - Duration: 0.29s\n",
      "Group 78: (60, 10) - Duration: 0.29s\n",
      "Group 79: (60, 10) - Duration: 0.30s\n",
      "Group 80: (60, 10) - Duration: 0.29s\n",
      "Group 81: (60, 10) - Duration: 0.30s\n",
      "Group 82: (60, 10) - Duration: 0.30s\n",
      "Group 83: (60, 10) - Duration: 0.30s\n",
      "Group 84: (60, 10) - Duration: 0.29s\n",
      "Group 85: (60, 10) - Duration: 0.30s\n",
      "Group 86: (60, 10) - Duration: 0.29s\n",
      "Group 87: (60, 10) - Duration: 0.29s\n",
      "Group 88: (60, 10) - Duration: 0.29s\n",
      "Group 89: (60, 10) - Duration: 0.30s\n",
      "Group 90: (60, 10) - Duration: 0.29s\n",
      "Group 91: (60, 10) - Duration: 0.29s\n",
      "Group 92: (60, 10) - Duration: 0.30s\n",
      "Group 93: (60, 10) - Duration: 0.29s\n",
      "Group 94: (60, 10) - Duration: 0.30s\n",
      "Group 95: (60, 10) - Duration: 0.30s\n",
      "Group 96: (60, 10) - Duration: 0.30s\n",
      "Group 97: (60, 10) - Duration: 0.29s\n",
      "Group 98: (60, 10) - Duration: 0.29s\n",
      "Group 99: (60, 10) - Duration: 0.30s\n",
      "Group 100: (60, 10) - Duration: 0.30s\n",
      "Group 101: (60, 10) - Duration: 0.30s\n",
      "Group 102: (60, 10) - Duration: 0.30s\n",
      "Group 103: (60, 10) - Duration: 0.30s\n",
      "Group 104: (60, 10) - Duration: 0.30s\n",
      "Group 105: (60, 10) - Duration: 0.30s\n",
      "Group 106: (60, 10) - Duration: 0.30s\n",
      "Group 107: (60, 10) - Duration: 0.30s\n",
      "Group 108: (60, 10) - Duration: 0.29s\n",
      "Group 109: (60, 10) - Duration: 0.30s\n",
      "Group 110: (60, 10) - Duration: 0.29s\n",
      "Group 111: (60, 10) - Duration: 0.29s\n",
      "Group 112: (60, 10) - Duration: 0.29s\n",
      "Group 113: (60, 10) - Duration: 0.29s\n",
      "Group 114: (60, 10) - Duration: 0.29s\n",
      "Group 115: (60, 10) - Duration: 0.30s\n",
      "Group 116: (60, 10) - Duration: 0.30s\n",
      "Group 117: (60, 10) - Duration: 0.30s\n",
      "Group 118: (60, 10) - Duration: 0.29s\n",
      "Group 119: (60, 10) - Duration: 0.30s\n",
      "Group 120: (60, 10) - Duration: 0.29s\n",
      "Group 121: (60, 10) - Duration: 0.29s\n",
      "Group 122: (60, 10) - Duration: 0.30s\n",
      "Group 123: (60, 10) - Duration: 0.29s\n",
      "Group 124: (60, 10) - Duration: 0.30s\n",
      "Group 125: (60, 10) - Duration: 0.29s\n",
      "\n",
      "Extracted Data Overview:\n",
      "Total records: 7560\n",
      "Number of groups: 126\n",
      "Columns: ['timestamp', 'button_press', 'gyro_pitch', 'gyro_yaw', 'gyro_roll', 'acc_x', 'acc_y', 'acc_z', 'user_id', 'audio_file', 'group_index']\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "                     timestamp button_press  gyro_pitch  gyro_yaw  gyro_roll  \\\n",
      "357 2025-03-28 18:18:58.238672         none         -18      7593      -3253   \n",
      "358 2025-03-28 18:18:58.243474         none         -22      7609      -3252   \n",
      "359 2025-03-28 18:18:58.248546         none         -33      7595      -3279   \n",
      "360 2025-03-28 18:18:58.253701         none         -42      7609      -3267   \n",
      "361 2025-03-28 18:18:58.258733         none        -104      7589      -3314   \n",
      "\n",
      "     acc_x  acc_y  acc_z           user_id  \\\n",
      "357     41      4     -3  user1_non-haptic   \n",
      "358     39      0     -7  user1_non-haptic   \n",
      "359     35      2     -7  user1_non-haptic   \n",
      "360     30      2     -7  user1_non-haptic   \n",
      "361     21      5     -7  user1_non-haptic   \n",
      "\n",
      "                                     audio_file  group_index  \n",
      "357  audio_user1_non-haptic_20250328_181856.wav            0  \n",
      "358  audio_user1_non-haptic_20250328_181856.wav            0  \n",
      "359  audio_user1_non-haptic_20250328_181856.wav            0  \n",
      "360  audio_user1_non-haptic_20250328_181856.wav            0  \n",
      "361  audio_user1_non-haptic_20250328_181856.wav            0  \n",
      "\n",
      "Button press distribution:\n",
      "button_press\n",
      "cross    4993\n",
      "none     2567\n",
      "Name: count, dtype: int64\n",
      "Extracted 98 action groups from ../data/fuse_one/controller_data_user2_non-haptic_20250327_152659.csv\n",
      "Shape of extracted data: (5880, 11)\n",
      "Total number of samples: 5880\n",
      "Number of unique groups: 98\n",
      "\n",
      "Shape of each sample (group):\n",
      "Group 0: (60, 10) - Duration: 0.30s\n",
      "Group 1: (60, 10) - Duration: 0.30s\n",
      "Group 2: (60, 10) - Duration: 0.30s\n",
      "Group 3: (60, 10) - Duration: 0.30s\n",
      "Group 4: (60, 10) - Duration: 0.29s\n",
      "Group 5: (60, 10) - Duration: 0.29s\n",
      "Group 6: (60, 10) - Duration: 0.30s\n",
      "Group 7: (60, 10) - Duration: 0.29s\n",
      "Group 8: (60, 10) - Duration: 0.30s\n",
      "Group 9: (60, 10) - Duration: 0.30s\n",
      "Group 10: (60, 10) - Duration: 0.29s\n",
      "Group 11: (60, 10) - Duration: 0.30s\n",
      "Group 12: (60, 10) - Duration: 0.30s\n",
      "Group 13: (60, 10) - Duration: 0.29s\n",
      "Group 14: (60, 10) - Duration: 0.29s\n",
      "Group 15: (60, 10) - Duration: 0.29s\n",
      "Group 16: (60, 10) - Duration: 0.30s\n",
      "Group 17: (60, 10) - Duration: 0.30s\n",
      "Group 18: (60, 10) - Duration: 0.30s\n",
      "Group 19: (60, 10) - Duration: 0.29s\n",
      "Group 20: (60, 10) - Duration: 0.29s\n",
      "Group 21: (60, 10) - Duration: 0.30s\n",
      "Group 22: (60, 10) - Duration: 0.29s\n",
      "Group 23: (60, 10) - Duration: 0.30s\n",
      "Group 24: (60, 10) - Duration: 0.29s\n",
      "Group 25: (60, 10) - Duration: 0.29s\n",
      "Group 26: (60, 10) - Duration: 0.30s\n",
      "Group 27: (60, 10) - Duration: 0.30s\n",
      "Group 28: (60, 10) - Duration: 0.29s\n",
      "Group 29: (60, 10) - Duration: 0.30s\n",
      "Group 30: (60, 10) - Duration: 0.33s\n",
      "Group 31: (60, 10) - Duration: 0.30s\n",
      "Group 32: (60, 10) - Duration: 0.29s\n",
      "Group 33: (60, 10) - Duration: 0.29s\n",
      "Group 34: (60, 10) - Duration: 0.29s\n",
      "Group 35: (60, 10) - Duration: 0.30s\n",
      "Group 36: (60, 10) - Duration: 0.29s\n",
      "Group 37: (60, 10) - Duration: 0.29s\n",
      "Group 38: (60, 10) - Duration: 0.30s\n",
      "Group 39: (60, 10) - Duration: 0.30s\n",
      "Group 40: (60, 10) - Duration: 0.29s\n",
      "Group 41: (60, 10) - Duration: 0.30s\n",
      "Group 42: (60, 10) - Duration: 0.30s\n",
      "Group 43: (60, 10) - Duration: 0.29s\n",
      "Group 44: (60, 10) - Duration: 0.29s\n",
      "Group 45: (60, 10) - Duration: 0.30s\n",
      "Group 46: (60, 10) - Duration: 0.29s\n",
      "Group 47: (60, 10) - Duration: 0.33s\n",
      "Group 48: (60, 10) - Duration: 0.29s\n",
      "Group 49: (60, 10) - Duration: 0.29s\n",
      "Group 50: (60, 10) - Duration: 0.29s\n",
      "Group 51: (60, 10) - Duration: 0.30s\n",
      "Group 52: (60, 10) - Duration: 0.29s\n",
      "Group 53: (60, 10) - Duration: 0.30s\n",
      "Group 54: (60, 10) - Duration: 0.29s\n",
      "Group 55: (60, 10) - Duration: 0.30s\n",
      "Group 56: (60, 10) - Duration: 0.30s\n",
      "Group 57: (60, 10) - Duration: 0.30s\n",
      "Group 58: (60, 10) - Duration: 0.30s\n",
      "Group 59: (60, 10) - Duration: 0.30s\n",
      "Group 60: (60, 10) - Duration: 0.29s\n",
      "Group 61: (60, 10) - Duration: 0.29s\n",
      "Group 62: (60, 10) - Duration: 0.30s\n",
      "Group 63: (60, 10) - Duration: 0.29s\n",
      "Group 64: (60, 10) - Duration: 0.30s\n",
      "Group 65: (60, 10) - Duration: 0.29s\n",
      "Group 66: (60, 10) - Duration: 0.30s\n",
      "Group 67: (60, 10) - Duration: 0.30s\n",
      "Group 68: (60, 10) - Duration: 0.29s\n",
      "Group 69: (60, 10) - Duration: 0.29s\n",
      "Group 70: (60, 10) - Duration: 0.29s\n",
      "Group 71: (60, 10) - Duration: 0.30s\n",
      "Group 72: (60, 10) - Duration: 0.29s\n",
      "Group 73: (60, 10) - Duration: 0.30s\n",
      "Group 74: (60, 10) - Duration: 0.29s\n",
      "Group 75: (60, 10) - Duration: 0.29s\n",
      "Group 76: (60, 10) - Duration: 0.29s\n",
      "Group 77: (60, 10) - Duration: 0.29s\n",
      "Group 78: (60, 10) - Duration: 0.30s\n",
      "Group 79: (60, 10) - Duration: 0.30s\n",
      "Group 80: (60, 10) - Duration: 0.30s\n",
      "Group 81: (60, 10) - Duration: 0.30s\n",
      "Group 82: (60, 10) - Duration: 0.29s\n",
      "Group 83: (60, 10) - Duration: 0.29s\n",
      "Group 84: (60, 10) - Duration: 0.30s\n",
      "Group 85: (60, 10) - Duration: 0.30s\n",
      "Group 86: (60, 10) - Duration: 0.29s\n",
      "Group 87: (60, 10) - Duration: 0.30s\n",
      "Group 88: (60, 10) - Duration: 0.30s\n",
      "Group 89: (60, 10) - Duration: 0.29s\n",
      "Group 90: (60, 10) - Duration: 0.30s\n",
      "Group 91: (60, 10) - Duration: 0.29s\n",
      "Group 92: (60, 10) - Duration: 0.29s\n",
      "Group 93: (60, 10) - Duration: 0.29s\n",
      "Group 94: (60, 10) - Duration: 0.29s\n",
      "Group 95: (60, 10) - Duration: 0.29s\n",
      "Group 96: (60, 10) - Duration: 0.30s\n",
      "Group 97: (60, 10) - Duration: 0.29s\n",
      "\n",
      "Extracted Data Overview:\n",
      "Total records: 5880\n",
      "Number of groups: 98\n",
      "Columns: ['timestamp', 'button_press', 'gyro_pitch', 'gyro_yaw', 'gyro_roll', 'acc_x', 'acc_y', 'acc_z', 'user_id', 'audio_file', 'group_index']\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "                     timestamp button_press  gyro_pitch  gyro_yaw  gyro_roll  \\\n",
      "284 2025-03-27 15:27:00.488128         none         897      7743       2373   \n",
      "285 2025-03-27 15:27:00.493182         none         912      7708       2374   \n",
      "286 2025-03-27 15:27:00.497803         none         908      7745       2372   \n",
      "287 2025-03-27 15:27:00.502876         none         863      7779       2367   \n",
      "288 2025-03-27 15:27:00.507933         none         904      7771       2348   \n",
      "\n",
      "     acc_x  acc_y  acc_z           user_id  \\\n",
      "284     -8     -3     -5  user2_non-haptic   \n",
      "285     -9     -4     -3  user2_non-haptic   \n",
      "286     -9     -3     -2  user2_non-haptic   \n",
      "287     -3     -2     -2  user2_non-haptic   \n",
      "288      0     -5     -1  user2_non-haptic   \n",
      "\n",
      "                                     audio_file  group_index  \n",
      "284  audio_user2_non-haptic_20250327_152659.wav            0  \n",
      "285  audio_user2_non-haptic_20250327_152659.wav            0  \n",
      "286  audio_user2_non-haptic_20250327_152659.wav            0  \n",
      "287  audio_user2_non-haptic_20250327_152659.wav            0  \n",
      "288  audio_user2_non-haptic_20250327_152659.wav            0  \n",
      "\n",
      "Button press distribution:\n",
      "button_press\n",
      "none     2992\n",
      "cross    2888\n",
      "Name: count, dtype: int64\n",
      "Extracted 101 action groups from ../data/fuse_one/controller_data_user3_non-haptic_20250327_160855.csv\n",
      "Shape of extracted data: (6060, 11)\n",
      "Total number of samples: 6060\n",
      "Number of unique groups: 101\n",
      "\n",
      "Shape of each sample (group):\n",
      "Group 0: (60, 10) - Duration: 0.30s\n",
      "Group 1: (60, 10) - Duration: 0.29s\n",
      "Group 2: (60, 10) - Duration: 0.29s\n",
      "Group 3: (60, 10) - Duration: 0.29s\n",
      "Group 4: (60, 10) - Duration: 0.29s\n",
      "Group 5: (60, 10) - Duration: 0.30s\n",
      "Group 6: (60, 10) - Duration: 0.30s\n",
      "Group 7: (60, 10) - Duration: 0.30s\n",
      "Group 8: (60, 10) - Duration: 0.29s\n",
      "Group 9: (60, 10) - Duration: 0.29s\n",
      "Group 10: (60, 10) - Duration: 0.30s\n",
      "Group 11: (60, 10) - Duration: 0.30s\n",
      "Group 12: (60, 10) - Duration: 0.30s\n",
      "Group 13: (60, 10) - Duration: 0.30s\n",
      "Group 14: (60, 10) - Duration: 0.30s\n",
      "Group 15: (60, 10) - Duration: 0.29s\n",
      "Group 16: (60, 10) - Duration: 0.30s\n",
      "Group 17: (60, 10) - Duration: 0.29s\n",
      "Group 18: (60, 10) - Duration: 0.30s\n",
      "Group 19: (60, 10) - Duration: 0.30s\n",
      "Group 20: (60, 10) - Duration: 0.29s\n",
      "Group 21: (60, 10) - Duration: 0.29s\n",
      "Group 22: (60, 10) - Duration: 0.29s\n",
      "Group 23: (60, 10) - Duration: 0.30s\n",
      "Group 24: (60, 10) - Duration: 0.29s\n",
      "Group 25: (60, 10) - Duration: 0.29s\n",
      "Group 26: (60, 10) - Duration: 0.29s\n",
      "Group 27: (60, 10) - Duration: 0.29s\n",
      "Group 28: (60, 10) - Duration: 0.30s\n",
      "Group 29: (60, 10) - Duration: 0.29s\n",
      "Group 30: (60, 10) - Duration: 0.30s\n",
      "Group 31: (60, 10) - Duration: 0.30s\n",
      "Group 32: (60, 10) - Duration: 0.30s\n",
      "Group 33: (60, 10) - Duration: 0.30s\n",
      "Group 34: (60, 10) - Duration: 0.29s\n",
      "Group 35: (60, 10) - Duration: 0.29s\n",
      "Group 36: (60, 10) - Duration: 0.29s\n",
      "Group 37: (60, 10) - Duration: 0.29s\n",
      "Group 38: (60, 10) - Duration: 0.29s\n",
      "Group 39: (60, 10) - Duration: 0.29s\n",
      "Group 40: (60, 10) - Duration: 0.32s\n",
      "Group 41: (60, 10) - Duration: 0.29s\n",
      "Group 42: (60, 10) - Duration: 0.29s\n",
      "Group 43: (60, 10) - Duration: 0.29s\n",
      "Group 44: (60, 10) - Duration: 0.30s\n",
      "Group 45: (60, 10) - Duration: 0.29s\n",
      "Group 46: (60, 10) - Duration: 0.29s\n",
      "Group 47: (60, 10) - Duration: 0.29s\n",
      "Group 48: (60, 10) - Duration: 0.30s\n",
      "Group 49: (60, 10) - Duration: 0.30s\n",
      "Group 50: (60, 10) - Duration: 0.30s\n",
      "Group 51: (60, 10) - Duration: 0.29s\n",
      "Group 52: (60, 10) - Duration: 0.29s\n",
      "Group 53: (60, 10) - Duration: 0.29s\n",
      "Group 54: (60, 10) - Duration: 0.30s\n",
      "Group 55: (60, 10) - Duration: 0.30s\n",
      "Group 56: (60, 10) - Duration: 0.29s\n",
      "Group 57: (60, 10) - Duration: 0.30s\n",
      "Group 58: (60, 10) - Duration: 0.29s\n",
      "Group 59: (60, 10) - Duration: 0.29s\n",
      "Group 60: (60, 10) - Duration: 0.29s\n",
      "Group 61: (60, 10) - Duration: 0.30s\n",
      "Group 62: (60, 10) - Duration: 0.30s\n",
      "Group 63: (60, 10) - Duration: 0.30s\n",
      "Group 64: (60, 10) - Duration: 0.30s\n",
      "Group 65: (60, 10) - Duration: 0.30s\n",
      "Group 66: (60, 10) - Duration: 0.29s\n",
      "Group 67: (60, 10) - Duration: 0.30s\n",
      "Group 68: (60, 10) - Duration: 0.29s\n",
      "Group 69: (60, 10) - Duration: 0.30s\n",
      "Group 70: (60, 10) - Duration: 0.30s\n",
      "Group 71: (60, 10) - Duration: 0.30s\n",
      "Group 72: (60, 10) - Duration: 0.29s\n",
      "Group 73: (60, 10) - Duration: 0.29s\n",
      "Group 74: (60, 10) - Duration: 0.29s\n",
      "Group 75: (60, 10) - Duration: 0.30s\n",
      "Group 76: (60, 10) - Duration: 0.30s\n",
      "Group 77: (60, 10) - Duration: 0.29s\n",
      "Group 78: (60, 10) - Duration: 0.30s\n",
      "Group 79: (60, 10) - Duration: 0.29s\n",
      "Group 80: (60, 10) - Duration: 0.30s\n",
      "Group 81: (60, 10) - Duration: 0.29s\n",
      "Group 82: (60, 10) - Duration: 0.29s\n",
      "Group 83: (60, 10) - Duration: 0.29s\n",
      "Group 84: (60, 10) - Duration: 0.29s\n",
      "Group 85: (60, 10) - Duration: 0.30s\n",
      "Group 86: (60, 10) - Duration: 0.30s\n",
      "Group 87: (60, 10) - Duration: 0.30s\n",
      "Group 88: (60, 10) - Duration: 0.29s\n",
      "Group 89: (60, 10) - Duration: 0.29s\n",
      "Group 90: (60, 10) - Duration: 0.29s\n",
      "Group 91: (60, 10) - Duration: 0.29s\n",
      "Group 92: (60, 10) - Duration: 0.30s\n",
      "Group 93: (60, 10) - Duration: 0.29s\n",
      "Group 94: (60, 10) - Duration: 0.30s\n",
      "Group 95: (60, 10) - Duration: 0.30s\n",
      "Group 96: (60, 10) - Duration: 0.29s\n",
      "Group 97: (60, 10) - Duration: 0.30s\n",
      "Group 98: (60, 10) - Duration: 0.30s\n",
      "Group 99: (60, 10) - Duration: 0.30s\n",
      "Group 100: (60, 10) - Duration: 0.30s\n",
      "\n",
      "Extracted Data Overview:\n",
      "Total records: 6060\n",
      "Number of groups: 101\n",
      "Columns: ['timestamp', 'button_press', 'gyro_pitch', 'gyro_yaw', 'gyro_roll', 'acc_x', 'acc_y', 'acc_z', 'user_id', 'audio_file', 'group_index']\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "                     timestamp button_press  gyro_pitch  gyro_yaw  gyro_roll  \\\n",
      "291 2025-03-27 16:08:57.256971         none         448      8284       -427   \n",
      "292 2025-03-27 16:08:57.262019         none         463      8277       -405   \n",
      "293 2025-03-27 16:08:57.267106         none         457      8223       -404   \n",
      "294 2025-03-27 16:08:57.272156         none         408      8249       -394   \n",
      "295 2025-03-27 16:08:57.277204         none         434      8223       -402   \n",
      "\n",
      "     acc_x  acc_y  acc_z           user_id  \\\n",
      "291    -10     -1     11  user3_non-haptic   \n",
      "292     -8     -2     10  user3_non-haptic   \n",
      "293     -7     -1      5  user3_non-haptic   \n",
      "294     -3      0      4  user3_non-haptic   \n",
      "295     -3      0      4  user3_non-haptic   \n",
      "\n",
      "                                     audio_file  group_index  \n",
      "291  audio_user3_non-haptic_20250327_160855.wav            0  \n",
      "292  audio_user3_non-haptic_20250327_160855.wav            0  \n",
      "293  audio_user3_non-haptic_20250327_160855.wav            0  \n",
      "294  audio_user3_non-haptic_20250327_160855.wav            0  \n",
      "295  audio_user3_non-haptic_20250327_160855.wav            0  \n",
      "\n",
      "Button press distribution:\n",
      "button_press\n",
      "cross    3901\n",
      "none     2159\n",
      "Name: count, dtype: int64\n",
      "Extracted 120 action groups from ../data/fuse_one/controller_data_user4_non-haptic_20250327_165510.csv\n",
      "Shape of extracted data: (7200, 11)\n",
      "Total number of samples: 7200\n",
      "Number of unique groups: 120\n",
      "\n",
      "Shape of each sample (group):\n",
      "Group 0: (60, 10) - Duration: 0.30s\n",
      "Group 1: (60, 10) - Duration: 0.29s\n",
      "Group 2: (60, 10) - Duration: 0.29s\n",
      "Group 3: (60, 10) - Duration: 0.29s\n",
      "Group 4: (60, 10) - Duration: 0.29s\n",
      "Group 5: (60, 10) - Duration: 0.29s\n",
      "Group 6: (60, 10) - Duration: 0.30s\n",
      "Group 7: (60, 10) - Duration: 0.30s\n",
      "Group 8: (60, 10) - Duration: 0.29s\n",
      "Group 9: (60, 10) - Duration: 0.30s\n",
      "Group 10: (60, 10) - Duration: 0.30s\n",
      "Group 11: (60, 10) - Duration: 0.29s\n",
      "Group 12: (60, 10) - Duration: 0.29s\n",
      "Group 13: (60, 10) - Duration: 0.30s\n",
      "Group 14: (60, 10) - Duration: 0.30s\n",
      "Group 15: (60, 10) - Duration: 0.29s\n",
      "Group 16: (60, 10) - Duration: 0.29s\n",
      "Group 17: (60, 10) - Duration: 0.29s\n",
      "Group 18: (60, 10) - Duration: 0.29s\n",
      "Group 19: (60, 10) - Duration: 0.30s\n",
      "Group 20: (60, 10) - Duration: 0.29s\n",
      "Group 21: (60, 10) - Duration: 0.30s\n",
      "Group 22: (60, 10) - Duration: 0.29s\n",
      "Group 23: (60, 10) - Duration: 0.29s\n",
      "Group 24: (60, 10) - Duration: 0.30s\n",
      "Group 25: (60, 10) - Duration: 0.30s\n",
      "Group 26: (60, 10) - Duration: 0.29s\n",
      "Group 27: (60, 10) - Duration: 0.30s\n",
      "Group 28: (60, 10) - Duration: 0.29s\n",
      "Group 29: (60, 10) - Duration: 0.31s\n",
      "Group 30: (60, 10) - Duration: 0.30s\n",
      "Group 31: (60, 10) - Duration: 0.29s\n",
      "Group 32: (60, 10) - Duration: 0.29s\n",
      "Group 33: (60, 10) - Duration: 0.29s\n",
      "Group 34: (60, 10) - Duration: 0.30s\n",
      "Group 35: (60, 10) - Duration: 0.30s\n",
      "Group 36: (60, 10) - Duration: 0.29s\n",
      "Group 37: (60, 10) - Duration: 0.30s\n",
      "Group 38: (60, 10) - Duration: 0.29s\n",
      "Group 39: (60, 10) - Duration: 0.30s\n",
      "Group 40: (60, 10) - Duration: 0.29s\n",
      "Group 41: (60, 10) - Duration: 0.29s\n",
      "Group 42: (60, 10) - Duration: 0.30s\n",
      "Group 43: (60, 10) - Duration: 0.30s\n",
      "Group 44: (60, 10) - Duration: 0.30s\n",
      "Group 45: (60, 10) - Duration: 0.30s\n",
      "Group 46: (60, 10) - Duration: 0.29s\n",
      "Group 47: (60, 10) - Duration: 0.30s\n",
      "Group 48: (60, 10) - Duration: 0.29s\n",
      "Group 49: (60, 10) - Duration: 0.30s\n",
      "Group 50: (60, 10) - Duration: 0.30s\n",
      "Group 51: (60, 10) - Duration: 0.29s\n",
      "Group 52: (60, 10) - Duration: 0.30s\n",
      "Group 53: (60, 10) - Duration: 0.30s\n",
      "Group 54: (60, 10) - Duration: 0.30s\n",
      "Group 55: (60, 10) - Duration: 0.29s\n",
      "Group 56: (60, 10) - Duration: 0.29s\n",
      "Group 57: (60, 10) - Duration: 0.29s\n",
      "Group 58: (60, 10) - Duration: 0.29s\n",
      "Group 59: (60, 10) - Duration: 0.30s\n",
      "Group 60: (60, 10) - Duration: 0.30s\n",
      "Group 61: (60, 10) - Duration: 0.30s\n",
      "Group 62: (60, 10) - Duration: 0.29s\n",
      "Group 63: (60, 10) - Duration: 0.30s\n",
      "Group 64: (60, 10) - Duration: 0.29s\n",
      "Group 65: (60, 10) - Duration: 0.29s\n",
      "Group 66: (60, 10) - Duration: 0.31s\n",
      "Group 67: (60, 10) - Duration: 0.30s\n",
      "Group 68: (60, 10) - Duration: 0.30s\n",
      "Group 69: (60, 10) - Duration: 0.30s\n",
      "Group 70: (60, 10) - Duration: 0.30s\n",
      "Group 71: (60, 10) - Duration: 0.29s\n",
      "Group 72: (60, 10) - Duration: 0.29s\n",
      "Group 73: (60, 10) - Duration: 0.29s\n",
      "Group 74: (60, 10) - Duration: 0.30s\n",
      "Group 75: (60, 10) - Duration: 0.30s\n",
      "Group 76: (60, 10) - Duration: 0.29s\n",
      "Group 77: (60, 10) - Duration: 0.29s\n",
      "Group 78: (60, 10) - Duration: 0.29s\n",
      "Group 79: (60, 10) - Duration: 0.30s\n",
      "Group 80: (60, 10) - Duration: 0.29s\n",
      "Group 81: (60, 10) - Duration: 0.29s\n",
      "Group 82: (60, 10) - Duration: 0.30s\n",
      "Group 83: (60, 10) - Duration: 0.29s\n",
      "Group 84: (60, 10) - Duration: 0.29s\n",
      "Group 85: (60, 10) - Duration: 0.30s\n",
      "Group 86: (60, 10) - Duration: 0.30s\n",
      "Group 87: (60, 10) - Duration: 0.29s\n",
      "Group 88: (60, 10) - Duration: 0.29s\n",
      "Group 89: (60, 10) - Duration: 0.30s\n",
      "Group 90: (60, 10) - Duration: 0.29s\n",
      "Group 91: (60, 10) - Duration: 0.29s\n",
      "Group 92: (60, 10) - Duration: 0.30s\n",
      "Group 93: (60, 10) - Duration: 0.29s\n",
      "Group 94: (60, 10) - Duration: 0.30s\n",
      "Group 95: (60, 10) - Duration: 0.29s\n",
      "Group 96: (60, 10) - Duration: 0.30s\n",
      "Group 97: (60, 10) - Duration: 0.29s\n",
      "Group 98: (60, 10) - Duration: 0.29s\n",
      "Group 99: (60, 10) - Duration: 0.29s\n",
      "Group 100: (60, 10) - Duration: 0.29s\n",
      "Group 101: (60, 10) - Duration: 0.29s\n",
      "Group 102: (60, 10) - Duration: 0.30s\n",
      "Group 103: (60, 10) - Duration: 0.29s\n",
      "Group 104: (60, 10) - Duration: 0.29s\n",
      "Group 105: (60, 10) - Duration: 0.29s\n",
      "Group 106: (60, 10) - Duration: 0.29s\n",
      "Group 107: (60, 10) - Duration: 0.29s\n",
      "Group 108: (60, 10) - Duration: 0.30s\n",
      "Group 109: (60, 10) - Duration: 0.29s\n",
      "Group 110: (60, 10) - Duration: 0.29s\n",
      "Group 111: (60, 10) - Duration: 0.30s\n",
      "Group 112: (60, 10) - Duration: 0.30s\n",
      "Group 113: (60, 10) - Duration: 0.30s\n",
      "Group 114: (60, 10) - Duration: 0.29s\n",
      "Group 115: (60, 10) - Duration: 0.31s\n",
      "Group 116: (60, 10) - Duration: 0.29s\n",
      "Group 117: (60, 10) - Duration: 0.29s\n",
      "Group 118: (60, 10) - Duration: 0.29s\n",
      "Group 119: (60, 10) - Duration: 0.30s\n",
      "\n",
      "Extracted Data Overview:\n",
      "Total records: 7200\n",
      "Number of groups: 120\n",
      "Columns: ['timestamp', 'button_press', 'gyro_pitch', 'gyro_yaw', 'gyro_roll', 'acc_x', 'acc_y', 'acc_z', 'user_id', 'audio_file', 'group_index']\n",
      "\n",
      "Sample data (first 5 rows):\n",
      "                     timestamp button_press  gyro_pitch  gyro_yaw  gyro_roll  \\\n",
      "321 2025-03-27 16:55:11.945903         none         607      8124        917   \n",
      "322 2025-03-27 16:55:11.950962         none         602      8136        912   \n",
      "323 2025-03-27 16:55:11.956014         none         590      8138        900   \n",
      "324 2025-03-27 16:55:11.961373         none         592      8157        926   \n",
      "325 2025-03-27 16:55:11.966416         none         582      8127        927   \n",
      "\n",
      "     acc_x  acc_y  acc_z           user_id  \\\n",
      "321      0     -2      9  user4_non-haptic   \n",
      "322     -1     -1      9  user4_non-haptic   \n",
      "323     -1      0      9  user4_non-haptic   \n",
      "324      0     -2     10  user4_non-haptic   \n",
      "325      0     -3     10  user4_non-haptic   \n",
      "\n",
      "                                     audio_file  group_index  \n",
      "321  audio_user4_non-haptic_20250327_165510.wav            0  \n",
      "322  audio_user4_non-haptic_20250327_165510.wav            0  \n",
      "323  audio_user4_non-haptic_20250327_165510.wav            0  \n",
      "324  audio_user4_non-haptic_20250327_165510.wav            0  \n",
      "325  audio_user4_non-haptic_20250327_165510.wav            0  \n",
      "\n",
      "Button press distribution:\n",
      "button_press\n",
      "none     3797\n",
      "cross    3403\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "u1_df,u1_g_time,u1_pro_time= extract_action_samples(user1_offh_csv)\n",
    "u2_df,u2_g_time,u2_pro_time=extract_action_samples(user2_offh_csv)\n",
    "u3_df,u3_g_time,u3_pro_time=extract_action_samples(user3_offh_csv)\n",
    "u4_df,u4_g_time,u4_pro_time=extract_action_samples(user4_offh_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 126 segments\n",
      "Groups with off-center peaks: [47, 49, 95, 117, 123]\n",
      "Number of low power peaks (abs peak < 0.05): 64\n",
      "Low power peak group indices: [0, 1, 3, 5, 10, 11, 13, 14, 17, 19, 21, 24, 27, 32, 34, 35, 37, 40, 42, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 58, 61, 71, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 98, 99, 102, 106, 107, 108, 109, 110, 115, 116, 117, 121, 122, 123, 124, 125]\n",
      "Percentage of low power peaks: 50.79%\n",
      "\n",
      "Peak amplitude statistics:\n",
      "Min peak amplitude: 0.003\n",
      "Max peak amplitude: 0.475\n",
      "Mean peak amplitude: 0.089\n",
      "\n",
      "Sample spectrogram shape frequency bins , number of time slices: 129 , 21\n",
      "Sample MFCC shape number of mfcc coefficients , number of time slices: 13 , 23\n",
      "\n",
      "\n",
      "Processed 98 segments\n",
      "Groups with off-center peaks: [49]\n",
      "Number of low power peaks (abs peak < 0.05): 98\n",
      "Low power peak group indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]\n",
      "Percentage of low power peaks: 100.00%\n",
      "\n",
      "Peak amplitude statistics:\n",
      "Min peak amplitude: 0.000\n",
      "Max peak amplitude: 0.012\n",
      "Mean peak amplitude: 0.001\n",
      "\n",
      "Sample spectrogram shape frequency bins , number of time slices: 129 , 21\n",
      "Sample MFCC shape number of mfcc coefficients , number of time slices: 13 , 23\n",
      "\n",
      "\n",
      "Processed 101 segments\n",
      "Groups with off-center peaks: [24]\n",
      "Number of low power peaks (abs peak < 0.05): 24\n",
      "Low power peak group indices: [0, 6, 7, 10, 12, 13, 16, 18, 19, 24, 27, 34, 36, 53, 59, 63, 74, 82, 86, 94, 96, 98, 99, 100]\n",
      "Percentage of low power peaks: 23.76%\n",
      "\n",
      "Peak amplitude statistics:\n",
      "Min peak amplitude: 0.008\n",
      "Max peak amplitude: 0.617\n",
      "Mean peak amplitude: 0.155\n",
      "\n",
      "Sample spectrogram shape frequency bins , number of time slices: 129 , 21\n",
      "Sample MFCC shape number of mfcc coefficients , number of time slices: 13 , 23\n",
      "\n",
      "\n",
      "Processed 120 segments\n",
      "Groups with off-center peaks: [22, 82, 91]\n",
      "Number of low power peaks (abs peak < 0.05): 34\n",
      "Low power peak group indices: [2, 6, 10, 22, 23, 24, 26, 32, 37, 38, 39, 49, 50, 51, 52, 53, 54, 56, 65, 70, 76, 79, 82, 91, 94, 95, 96, 98, 100, 101, 103, 107, 108, 117]\n",
      "Percentage of low power peaks: 28.33%\n",
      "\n",
      "Peak amplitude statistics:\n",
      "Min peak amplitude: 0.003\n",
      "Max peak amplitude: 0.875\n",
      "Mean peak amplitude: 0.192\n",
      "\n",
      "Sample spectrogram shape frequency bins , number of time slices: 129 , 21\n",
      "Sample MFCC shape number of mfcc coefficients , number of time slices: 13 , 23\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "u1_audio_splited=split_audio_by_timestamps(user1_offh_wav,u1_g_time,u1_pro_time,plot_spectrograms=False)\n",
    "u2_audio_splited=split_audio_by_timestamps(user2_offh_wav,u2_g_time,u2_pro_time,plot_spectrograms=False)\n",
    "u3_audio_splited=split_audio_by_timestamps(user3_offh_wav,u3_g_time,u3_pro_time,plot_spectrograms=False)\n",
    "u4_audio_splited=split_audio_by_timestamps(user4_offh_wav,u4_g_time,u4_pro_time,plot_spectrograms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare the data \n",
    "def calculate_derivative_rms_features(data, window_size=5):\n",
    "    \"\"\"\n",
    "    Calculate RMS features for derivatives using sliding windows\n",
    "    Args:\n",
    "        data: numpy array of shape (N, 6) - sequence with 6 features\n",
    "        window_size: size of window for RMS calculation (default=5)\n",
    "    Returns:\n",
    "        numpy array of RMS values for each complete window\n",
    "    \"\"\"\n",
    "    n_windows = len(data) // window_size\n",
    "    if len(data) % window_size > 0:  # Handle remaining data\n",
    "        n_windows += 1\n",
    "    \n",
    "    rms_features = []\n",
    "    for i in range(n_windows):\n",
    "        start_idx = i * window_size\n",
    "        end_idx = min(start_idx + window_size, len(data))\n",
    "        window_data = data[start_idx:end_idx, :]\n",
    "        \n",
    "        # Calculate RMS for each feature in this window\n",
    "        rms = np.sqrt(np.mean(np.square(window_data), axis=0))\n",
    "        rms_features.append(rms)\n",
    "    \n",
    "    return np.array(rms_features)  # Shape: (n_windows, 6)\n",
    "\n",
    "def prepare_derivative_features(data):\n",
    "    \"\"\"\n",
    "    Calculate first and second derivatives and their RMS features\n",
    "    Args:\n",
    "        data: numpy array of shape (60, 6) - one sample/sequence with 6 features\n",
    "    Returns:\n",
    "        combined RMS features from both derivatives\n",
    "        shape: (12, 12) - 12 time steps, 12 features (6 from each derivative)\n",
    "    \"\"\"\n",
    "    # Apply z-score normalization to the entire sequence first\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0) + 1e-9\n",
    "    normalized_data = (data - mean) / std\n",
    "    \n",
    "    # Calculate first derivative from normalized data\n",
    "    first_derivative = np.diff(normalized_data, axis=0)  # Shape: (59, 6)\n",
    "    \n",
    "    # Calculate second derivative\n",
    "    second_derivative = np.diff(first_derivative, axis=0)  # Shape: (58, 6)\n",
    "    \n",
    "    # Calculate RMS features for both derivatives\n",
    "    first_der_rms = calculate_derivative_rms_features(first_derivative)  # Shape: (~12, 6)\n",
    "    second_der_rms = calculate_derivative_rms_features(second_derivative)  # Shape: (~12, 6)\n",
    "    \n",
    "    # Ensure both have exactly 12 windows by padding or truncating\n",
    "    target_length = 12\n",
    "    \n",
    "    def adjust_length(features, target_len):\n",
    "        if len(features) > target_len:\n",
    "            return features[:target_len]\n",
    "        elif len(features) < target_len:\n",
    "            padding = np.zeros((target_len - len(features), features.shape[1]))\n",
    "            return np.vstack([features, padding])\n",
    "        return features\n",
    "    \n",
    "    first_der_rms = adjust_length(first_der_rms, target_length)\n",
    "    second_der_rms = adjust_length(second_der_rms, target_length)\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = np.hstack([first_der_rms, second_der_rms])  # Shape: (12, 12)\n",
    "    \n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class UserData:\n",
    "    \"\"\"Data class to store user-specific data components\"\"\"\n",
    "    df: pd.DataFrame\n",
    "    audio_splited: Dict[str, List]\n",
    "    meg_data: Optional[Dict] = None\n",
    "\n",
    "class PlaySenseDataManager:\n",
    "    \"\"\"\n",
    "    A comprehensive data manager for PlaySense data that handles:\n",
    "    1. Data merging from different sources\n",
    "    2. Dataset creation for PyTorch\n",
    "    3. Data splitting and loader creation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.users_data = {}\n",
    "        self.merged_data = {}\n",
    "        self.dataset = None\n",
    "    \n",
    "    def add_user_data(self, user_id: int, user_data: UserData) -> None:\n",
    "        \"\"\"Add data for a specific user\"\"\"\n",
    "        self.users_data[user_id] = user_data\n",
    "        \n",
    "    def merge_all_users_data(self) -> None:\n",
    "        \"\"\"Merge data for all added users\"\"\"\n",
    "        for user_id, user_data in self.users_data.items():\n",
    "            merged = self._merge_action_and_audio_data(\n",
    "                df=user_data.df,\n",
    "                audio_samples=user_data.audio_splited['audio_segments'],\n",
    "                peak_samples=user_data.audio_splited['peak_segments'],\n",
    "                spec_samples=user_data.audio_splited['spectrograms'],\n",
    "                mfcc_samples=user_data.audio_splited['mfccs'],\n",
    "                low_power_peaks=user_data.audio_splited['low_power_peaks']\n",
    "            )\n",
    "            self.merged_data[user_id] = merged\n",
    "            print(f\"\\nUser {user_id} data merged successfully:\")\n",
    "            print(f\"Number of groups: {len(merged)}\")\n",
    "            \n",
    "            # Print example shapes for the first group\n",
    "            if merged:\n",
    "                first_group_idx = list(merged.keys())[0]\n",
    "                first_group = merged[first_group_idx]\n",
    "                \n",
    "                # Debug information to check what's in first_group\n",
    "                print(f\"DEBUG - First group keys: {list(first_group.keys())}\")\n",
    "                print(f\"DEBUG - Action data keys: {list(first_group['action_data'].keys())}\")\n",
    "                print(f\"DEBUG - Inertial features keys: {list(first_group['action_data']['inertial_features'].keys())}\")\n",
    "                \n",
    "                # Check if derivative_rms is None before accessing shape\n",
    "                derivative_rms = first_group['action_data']['inertial_features']['derivative_rms']\n",
    "                if derivative_rms is None:\n",
    "                    print(f\"ERROR: derivative_rms is None for user {user_id}, group {first_group_idx}\")\n",
    "                    print(f\"DEBUG - Check _prepare_derivative_features implementation\")\n",
    "                    continue\n",
    "                \n",
    "                inertial_shape = derivative_rms.shape\n",
    "                \n",
    "                # Check if mfcc is None before accessing shape\n",
    "                mfcc = first_group['audio_data']['mfcc']\n",
    "                if mfcc is None:\n",
    "                    print(f\"ERROR: mfcc is None for user {user_id}, group {first_group_idx}\")\n",
    "                    continue\n",
    "                \n",
    "                mfcc_shape = mfcc.shape\n",
    "                \n",
    "                print(f\"Example shapes from first group (group {first_group_idx}):\")\n",
    "                print(f\"  - Inertial features shape: {inertial_shape}\")\n",
    "                print(f\"  - MFCC features shape: {mfcc_shape}\")\n",
    "\n",
    "    def _merge_action_and_audio_data(self, df, audio_samples, peak_samples, \n",
    "                                   spec_samples, mfcc_samples, low_power_peaks):\n",
    "        \"\"\"Internal method for merging action and audio data\"\"\"\n",
    "        # Verify input sample counts\n",
    "        group_indices = df['group_index'].unique()\n",
    "        n_groups = len(group_indices)\n",
    "        \n",
    "        # Initialize the merged dataset\n",
    "        merged_data = {}\n",
    "        \n",
    "        # Inertial data columns to extract\n",
    "        inertial_columns = ['gyro_pitch', 'gyro_yaw', 'gyro_roll', \n",
    "                           'acc_x', 'acc_y', 'acc_z']\n",
    "        \n",
    "        # Process each group\n",
    "        for group_idx in group_indices:\n",
    "            if int(group_idx) in low_power_peaks:\n",
    "                continue\n",
    "                \n",
    "            group_df = df[df['group_index'] == group_idx].copy()\n",
    "            inertial_data = group_df[inertial_columns].values\n",
    "            \n",
    "            # Debug information before calling _prepare_derivative_features\n",
    "            print(f\"DEBUG - Processing group {group_idx}\")\n",
    "            print(f\"DEBUG - Inertial data shape: {inertial_data.shape}\")\n",
    "            \n",
    "            # Check if inertial_data is valid\n",
    "            if inertial_data.size == 0:\n",
    "                print(f\"WARNING: Empty inertial data for group {group_idx}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            derivative_features = self._prepare_derivative_features(inertial_data)\n",
    "            \n",
    "            # Debug information after calling _prepare_derivative_features\n",
    "            if derivative_features is None:\n",
    "                print(f\"ERROR: _prepare_derivative_features returned None for group {group_idx}\")\n",
    "                continue\n",
    "                \n",
    "            # Check if audio data exists for this group\n",
    "            # Since audio_samples and mfcc_samples are arrays, we need to check differently\n",
    "            group_idx_int = int(group_idx)\n",
    "            audio_idx = np.where(np.array([i == group_idx_int for i in range(len(audio_samples))]))[0]\n",
    "            mfcc_idx = np.where(np.array([i == group_idx_int for i in range(len(mfcc_samples))]))[0]\n",
    "            \n",
    "            if len(audio_idx) == 0:\n",
    "                print(f\"WARNING: No audio data for group {group_idx}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            if len(mfcc_idx) == 0:\n",
    "                print(f\"WARNING: No MFCC data for group {group_idx}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            merged_data[int(group_idx)] = {\n",
    "                'action_data': {\n",
    "                    'dataframe': group_df,\n",
    "                    'inertial_features': {\n",
    "                        'derivative_rms': derivative_features,\n",
    "                        'feature_names': {\n",
    "                            'first_derivative': [f'first_der_rms_{col}' for col in inertial_columns],\n",
    "                            'second_derivative': [f'second_der_rms_{col}' for col in inertial_columns]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'audio_data': {\n",
    "                    'raw_audio': audio_samples[audio_idx[0]],\n",
    "                    'peak_audio': peak_samples[audio_idx[0]],\n",
    "                    'spectrogram': spec_samples[audio_idx[0]],\n",
    "                    'mfcc': mfcc_samples[mfcc_idx[0]],\n",
    "                    'is_low_power': False\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return merged_data\n",
    "\n",
    "    def _prepare_derivative_features(self, inertial_data):\n",
    "        \"\"\"Prepare derivative features from inertial data\"\"\"\n",
    "        # Debug information\n",
    "        print(f\"DEBUG - Inside _prepare_derivative_features\")\n",
    "        print(f\"DEBUG - Input data shape: {inertial_data.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # Call the external prepare_derivative_features function\n",
    "            # This is likely referring to the function defined outside this class\n",
    "            return prepare_derivative_features(inertial_data)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in _prepare_derivative_features: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def create_dataset(self) -> None:\n",
    "        \"\"\"Create PyTorch dataset from merged data\"\"\"\n",
    "        self.dataset = PlaySenseDataset(\n",
    "            data_dict=self.merged_data\n",
    "        )\n",
    "\n",
    "    def create_data_loaders(self, \n",
    "                          batch_size: int = 32,\n",
    "                          train_ratio: float = 0.8,\n",
    "                          shuffle: bool = True) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Create train and validation data loaders\"\"\"\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset not created. Call create_dataset() first.\")\n",
    "\n",
    "        # Calculate split sizes\n",
    "        train_size = int(train_ratio * len(self.dataset))\n",
    "        val_size = len(self.dataset) - train_size\n",
    "\n",
    "        # Split dataset\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            self.dataset, \n",
    "            [train_size, val_size]\n",
    "        )\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "    \n",
    "class PlaySenseDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for PlaySense data\"\"\"\n",
    "    def __init__(self, data_dict: Dict):\n",
    "        self.inertial_features = []\n",
    "        self.mfcc_features = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Print initial data statistics\n",
    "        print(\"\\nInitializing Dataset:\")\n",
    "        print(f\"Number of users: {len(data_dict)}\")\n",
    "        \n",
    "        # Create label mapping (user_id -> consecutive index)\n",
    "        unique_user_ids = sorted(list(data_dict.keys()))\n",
    "        self.label_mapping = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "        self.inverse_mapping = {idx: user_id for user_id, idx in self.label_mapping.items()}\n",
    "        \n",
    "        print(\"\\nLabel mapping (user_id -> model_label):\")\n",
    "        for user_id, idx in self.label_mapping.items():\n",
    "            print(f\"User {user_id} -> Label {idx}\")\n",
    "        \n",
    "        # Track samples statistics\n",
    "        samples_stats = {user_id: {'kept': 0, 'discarded': 0} for user_id in data_dict.keys()}\n",
    "        discarded_groups = {user_id: [] for user_id in data_dict.keys()}\n",
    "        \n",
    "        for user_id, user_data in data_dict.items():\n",
    "            if user_data is None or len(user_data) == 0:\n",
    "                print(f\"Warning: Empty or None data for user {user_id}\")\n",
    "                continue\n",
    "            \n",
    "            for group_idx in user_data:\n",
    "                group = user_data[group_idx]\n",
    "                \n",
    "                try:\n",
    "                    # Get MFCC features and check shape\n",
    "                    mfcc = group['audio_data']['mfcc']\n",
    "                    \n",
    "                    # Only keep samples with shape (13, 23)\n",
    "                    if not isinstance(mfcc, np.ndarray) or mfcc.shape != (13, 23):\n",
    "                        samples_stats[user_id]['discarded'] += 1\n",
    "                        discarded_groups[user_id].append(group_idx)\n",
    "                        continue\n",
    "                    \n",
    "                    # Get inertial features\n",
    "                    inertial = group['action_data']['inertial_features']['derivative_rms']\n",
    "                    \n",
    "                    # Verify inertial feature type\n",
    "                    if not isinstance(inertial, np.ndarray):\n",
    "                        print(f\"Warning: Invalid inertial feature type for user {user_id}, group {group_idx}\")\n",
    "                        continue\n",
    "                    \n",
    "                    self.inertial_features.append(inertial)\n",
    "                    self.mfcc_features.append(mfcc)\n",
    "                    # Map user_id to consecutive index\n",
    "                    self.labels.append(self.label_mapping[user_id])\n",
    "                    samples_stats[user_id]['kept'] += 1\n",
    "                    \n",
    "                except KeyError as e:\n",
    "                    print(f\"Warning: Missing data structure for user {user_id}, group {group_idx}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing user {user_id}, group {group_idx}: {e}\")\n",
    "        \n",
    "        # Print detailed statistics\n",
    "        print(\"\\nSampling Statistics:\")\n",
    "        print(\"-------------------\")\n",
    "        for user_id in samples_stats:\n",
    "            stats = samples_stats[user_id]\n",
    "            print(f\"\\nUser {user_id} (Label {self.label_mapping[user_id]}):\")\n",
    "            print(f\"  Kept samples: {stats['kept']}\")\n",
    "            print(f\"  Discarded samples: {stats['discarded']}\")\n",
    "            if discarded_groups[user_id]:\n",
    "                print(f\"  Discarded group indices: {discarded_groups[user_id]}\")\n",
    "        \n",
    "        # Convert to tensors if we have data\n",
    "        if self.inertial_features and self.mfcc_features:\n",
    "            try:\n",
    "                self.inertial_features = torch.FloatTensor(np.array(self.inertial_features))\n",
    "                self.mfcc_features = torch.FloatTensor(np.array(self.mfcc_features))\n",
    "                self.labels = torch.LongTensor(self.labels)\n",
    "                \n",
    "                # Print final dataset statistics\n",
    "                print(\"\\nFinal Dataset Statistics:\")\n",
    "                print(\"------------------------\")\n",
    "                print(f\"Total samples: {len(self.labels)}\")\n",
    "                class_dist = Counter(self.labels.numpy())\n",
    "                print(\"\\nClass distribution:\")\n",
    "                for label, count in sorted(class_dist.items()):\n",
    "                    original_id = self.inverse_mapping[label]\n",
    "                    print(f\"User {original_id} (Label {label}): {count} samples\")\n",
    "                print(f\"\\nFeature shapes:\")\n",
    "                print(f\"Inertial features: {self.inertial_features.shape}\")\n",
    "                print(f\"MFCC features: {self.mfcc_features.shape}\")\n",
    "                \n",
    "                # Verify label range\n",
    "                min_label = self.labels.min().item()\n",
    "                max_label = self.labels.max().item()\n",
    "                num_classes = len(self.label_mapping)\n",
    "                print(f\"\\nLabel range: [{min_label}, {max_label}] (Number of classes: {num_classes})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error converting to tensors: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            raise ValueError(\"No valid samples found in the dataset\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'inertial': self.inertial_features[idx],\n",
    "            'mfcc': self.mfcc_features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        \"\"\"Return the number of classes in the dataset\"\"\"\n",
    "        return len(self.label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize model\\nmodel = InertialMFCCTransformer(\\n    inertial_dim=12,     # Input dimension for inertial data\\n    mfcc_dim=13,         # Input dimension for MFCC data\\n    num_classes=3,       # Number of classes to predict\\n    d_model=64,          # Hidden dimension size\\n    nhead=4,             # Number of attention heads\\n    num_layers=2,        # Number of transformer layers\\n    dropout=0.1          # Dropout rate\\n)\\n\\n# Forward pass example\\nbatch_size = 32\\ninertial_input = torch.randn(batch_size, 12, 12)    # Batch of inertial data\\nmfcc_input = torch.randn(batch_size, 13, 23)        # Batch of MFCC data\\noutput = model(inertial_input, mfcc_input)          # Shape: (batch_size, num_classes)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InertialMFCCTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 inertial_dim=12,      # Inertial features dimension (12 for your 12x12 data)\n",
    "                 mfcc_dim=13,          # MFCC features dimension (13 for your 13x23 data)\n",
    "                 num_classes=3,         # Number of output classes\n",
    "                 d_model=64,           # Hidden dimension\n",
    "                 nhead=4,              # Number of attention heads\n",
    "                 num_layers=2,         # Number of transformer layers\n",
    "                 dropout=0.1):         # Dropout rate\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Inertial Branch\n",
    "        # Convert inertial features to transformer dimension\n",
    "        self.inertial_embedding = nn.Sequential(\n",
    "            nn.Linear(inertial_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Inertial transformer encoder\n",
    "        self.inertial_transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=d_model * 4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # 2. MFCC Branch\n",
    "        # Convert MFCC features to transformer dimension\n",
    "        self.mfcc_embedding = nn.Sequential(\n",
    "            nn.Linear(mfcc_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # MFCC transformer encoder\n",
    "        self.mfcc_transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=d_model * 4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # 3. Fusion Layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # 4. Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.LayerNorm(d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inertial_data, mfcc_data):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "            inertial_data: tensor of shape (batch_size, 12, 12)\n",
    "            mfcc_data: tensor of shape (batch_size, 13, 23)\n",
    "        Returns:\n",
    "            output: tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # 1. Process Inertial Data\n",
    "        # Transform and encode inertial features\n",
    "        x_inertial = self.inertial_embedding(inertial_data)     # (batch_size, 12, d_model)\n",
    "        x_inertial = self.inertial_transformer(x_inertial)      # (batch_size, 12, d_model)\n",
    "        # Global average pooling\n",
    "        x_inertial = torch.mean(x_inertial, dim=1)             # (batch_size, d_model)\n",
    "        \n",
    "        # 2. Process MFCC Data\n",
    "        # Transpose MFCC data to handle time dimension\n",
    "        x_mfcc = self.mfcc_embedding(mfcc_data.transpose(1, 2)) # (batch_size, 23, d_model)\n",
    "        x_mfcc = self.mfcc_transformer(x_mfcc)                  # (batch_size, 23, d_model)\n",
    "        # Global average pooling\n",
    "        x_mfcc = torch.mean(x_mfcc, dim=1)                     # (batch_size, d_model)\n",
    "        \n",
    "        # 3. Mid-Fusion\n",
    "        # Concatenate features from both modalities\n",
    "        x_combined = torch.cat([x_inertial, x_mfcc], dim=1)    # (batch_size, d_model*2)\n",
    "        x_fused = self.fusion(x_combined)                      # (batch_size, d_model)\n",
    "        \n",
    "        # 4. Classification\n",
    "        output = self.classifier(x_fused)                      # (batch_size, num_classes)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize model\n",
    "model = InertialMFCCTransformer(\n",
    "    inertial_dim=12,     # Input dimension for inertial data\n",
    "    mfcc_dim=13,         # Input dimension for MFCC data\n",
    "    num_classes=3,       # Number of classes to predict\n",
    "    d_model=64,          # Hidden dimension size\n",
    "    nhead=4,             # Number of attention heads\n",
    "    num_layers=2,        # Number of transformer layers\n",
    "    dropout=0.1          # Dropout rate\n",
    ")\n",
    "\n",
    "# Forward pass example\n",
    "batch_size = 32\n",
    "inertial_input = torch.randn(batch_size, 12, 12)    # Batch of inertial data\n",
    "mfcc_input = torch.randn(batch_size, 13, 23)        # Batch of MFCC data\n",
    "output = model(inertial_input, mfcc_input)          # Shape: (batch_size, num_classes)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class PlaySenseTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for PlaySense model that handles:\n",
    "    1. Training loop\n",
    "    2. Validation\n",
    "    3. Metrics tracking\n",
    "    4. Model checkpointing\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        data_manager,\n",
    "        learning_rate=0.001,\n",
    "        num_epochs=50,\n",
    "        device=None,\n",
    "        checkpoint_dir='checkpoints'\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.data_manager = data_manager\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Initialize criterion and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Initialize best metrics\n",
    "        self.best_val_accuracy = 0.0\n",
    "        \n",
    "        # Metrics history\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "            'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "        }\n",
    "\n",
    "    def train(self, batch_size=32, train_ratio=0.8):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"Starting training on device: {self.device}\")\n",
    "        print(f\"Model architecture:\\n{self.model}\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader, val_loader = self.data_manager.create_data_loaders(\n",
    "            batch_size=batch_size,\n",
    "            train_ratio=train_ratio\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Training phase\n",
    "            train_metrics = self._train_epoch(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_metrics = self._validate(val_loader)\n",
    "            \n",
    "            # Update history\n",
    "            self._update_history(train_metrics, val_metrics)\n",
    "            \n",
    "            # Print metrics\n",
    "            self._print_metrics(epoch, train_metrics, val_metrics)\n",
    "            \n",
    "            # Save checkpoint if best model\n",
    "            if val_metrics['accuracy'] > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = val_metrics['accuracy']\n",
    "                self._save_checkpoint(epoch, val_metrics)\n",
    "\n",
    "    def _train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            inertial = batch['inertial'].to(self.device)\n",
    "            mfcc = batch['mfcc'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inertial, mfcc)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            running_loss += loss.item()\n",
    "            predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self._calculate_metrics(predictions, true_labels, running_loss, len(train_loader))\n",
    "        return metrics\n",
    "\n",
    "    def _validate(self, val_loader):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                inertial = batch['inertial'].to(self.device)\n",
    "                mfcc = batch['mfcc'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inertial, mfcc)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                # Track metrics\n",
    "                running_loss += loss.item()\n",
    "                predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self._calculate_metrics(predictions, true_labels, running_loss, len(val_loader))\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_metrics(self, predictions, true_labels, running_loss, num_batches):\n",
    "        \"\"\"Calculate training/validation metrics\"\"\"\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            true_labels, predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'loss': running_loss / num_batches,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    def _update_history(self, train_metrics, val_metrics):\n",
    "        \"\"\"Update metrics history\"\"\"\n",
    "        self.history['train_loss'].append(train_metrics['loss'])\n",
    "        self.history['train_acc'].append(train_metrics['accuracy'])\n",
    "        self.history['train_f1'].append(train_metrics['f1'])\n",
    "        self.history['val_loss'].append(val_metrics['loss'])\n",
    "        self.history['val_acc'].append(val_metrics['accuracy'])\n",
    "        self.history['val_f1'].append(val_metrics['f1'])\n",
    "\n",
    "    def _print_metrics(self, epoch, train_metrics, val_metrics):\n",
    "        \"\"\"Print current metrics\"\"\"\n",
    "        print(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
    "        print(\"Training Metrics:\")\n",
    "        print(f\"Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1: {train_metrics['f1']:.4f}, Precision: {train_metrics['precision']:.4f}\")\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        print(f\"Loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1: {val_metrics['f1']:.4f}, Precision: {val_metrics['precision']:.4f}\")\n",
    "\n",
    "    def _save_checkpoint(self, epoch, metrics):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        checkpoint_path = os.path.join(\n",
    "            self.checkpoint_dir,\n",
    "            f'model_epoch{epoch}_acc{metrics[\"accuracy\"]:.4f}_{timestamp}.pth'\n",
    "        )\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'metrics': metrics,\n",
    "            'history': self.history\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"\\nCheckpoint saved: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Processing group 2\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 4\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 6\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 7\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 8\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 9\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 12\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 15\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 16\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 18\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 20\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 22\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 23\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 25\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 26\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 28\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 29\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 30\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 31\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 33\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 36\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 38\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 39\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 41\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 43\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 44\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 45\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 52\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 57\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 59\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 60\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 62\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 63\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 64\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 65\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 66\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 67\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 68\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 69\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 70\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 72\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 73\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 77\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 88\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 89\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 90\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 91\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 92\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 93\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 97\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 100\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 101\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 103\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 104\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 105\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 111\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 112\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 113\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 114\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 118\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 119\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 120\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "\n",
      "User 1 data merged successfully:\n",
      "Number of groups: 62\n",
      "DEBUG - First group keys: ['action_data', 'audio_data']\n",
      "DEBUG - Action data keys: ['dataframe', 'inertial_features']\n",
      "DEBUG - Inertial features keys: ['derivative_rms', 'feature_names']\n",
      "Example shapes from first group (group 2):\n",
      "  - Inertial features shape: (12, 12)\n",
      "  - MFCC features shape: (13, 23)\n",
      "DEBUG - Processing group 1\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 2\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 3\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 4\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 5\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 8\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 9\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 11\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 14\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 15\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 17\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 20\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 21\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 22\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 23\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 25\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 26\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 28\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 29\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 30\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 31\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 32\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 33\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 35\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 37\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 38\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 39\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 40\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 41\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 42\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 43\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 44\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 45\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 46\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 47\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 48\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 49\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 50\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 51\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 52\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 54\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 55\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 56\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 57\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 58\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 60\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 61\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 62\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 64\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 65\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 66\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 67\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 68\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 69\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 70\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 71\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 72\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 73\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 75\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 76\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 77\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 78\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 79\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 80\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 81\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 83\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 84\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 85\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 87\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 88\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 89\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 90\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 91\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 92\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 93\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 95\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 97\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "\n",
      "User 3 data merged successfully:\n",
      "Number of groups: 77\n",
      "DEBUG - First group keys: ['action_data', 'audio_data']\n",
      "DEBUG - Action data keys: ['dataframe', 'inertial_features']\n",
      "DEBUG - Inertial features keys: ['derivative_rms', 'feature_names']\n",
      "Example shapes from first group (group 1):\n",
      "  - Inertial features shape: (12, 12)\n",
      "  - MFCC features shape: (13, 23)\n",
      "DEBUG - Processing group 0\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 1\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 3\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 4\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 5\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 7\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 8\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 9\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 11\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 12\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 13\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 14\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 15\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 16\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 17\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 18\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 19\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 20\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 21\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 25\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 27\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 28\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 29\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 30\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 31\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 33\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 34\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 35\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 36\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 40\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 41\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 42\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 43\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 44\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 45\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 46\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 47\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 48\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 55\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 57\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 58\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 59\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 60\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 61\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 62\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 63\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 64\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 66\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 67\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 68\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 69\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 71\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 72\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 73\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 74\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 75\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 77\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 78\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 80\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 81\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 83\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 84\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 85\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 86\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 87\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 88\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 89\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 90\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 92\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 93\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 97\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 99\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 102\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 104\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 105\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 106\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 109\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 110\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 111\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 112\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 113\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 114\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 115\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 116\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 118\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "DEBUG - Processing group 119\n",
      "DEBUG - Inertial data shape: (60, 6)\n",
      "DEBUG - Inside _prepare_derivative_features\n",
      "DEBUG - Input data shape: (60, 6)\n",
      "\n",
      "User 4 data merged successfully:\n",
      "Number of groups: 86\n",
      "DEBUG - First group keys: ['action_data', 'audio_data']\n",
      "DEBUG - Action data keys: ['dataframe', 'inertial_features']\n",
      "DEBUG - Inertial features keys: ['derivative_rms', 'feature_names']\n",
      "Example shapes from first group (group 0):\n",
      "  - Inertial features shape: (12, 12)\n",
      "  - MFCC features shape: (13, 23)\n",
      "\n",
      "Initializing Dataset:\n",
      "Number of users: 3\n",
      "\n",
      "Label mapping (user_id -> model_label):\n",
      "User 1 -> Label 0\n",
      "User 3 -> Label 1\n",
      "User 4 -> Label 2\n",
      "\n",
      "Sampling Statistics:\n",
      "-------------------\n",
      "\n",
      "User 1 (Label 0):\n",
      "  Kept samples: 62\n",
      "  Discarded samples: 0\n",
      "\n",
      "User 3 (Label 1):\n",
      "  Kept samples: 77\n",
      "  Discarded samples: 0\n",
      "\n",
      "User 4 (Label 2):\n",
      "  Kept samples: 82\n",
      "  Discarded samples: 4\n",
      "  Discarded group indices: [4, 89, 90, 106]\n",
      "\n",
      "Final Dataset Statistics:\n",
      "------------------------\n",
      "Total samples: 221\n",
      "\n",
      "Class distribution:\n",
      "User 1 (Label 0): 62 samples\n",
      "User 3 (Label 1): 77 samples\n",
      "User 4 (Label 2): 82 samples\n",
      "\n",
      "Feature shapes:\n",
      "Inertial features: torch.Size([221, 12, 12])\n",
      "MFCC features: torch.Size([221, 13, 23])\n",
      "\n",
      "Label range: [0, 2] (Number of classes: 3)\n",
      "Starting training on device: cpu\n",
      "Model architecture:\n",
      "InertialMFCCTransformer(\n",
      "  (inertial_embedding): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (inertial_transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mfcc_embedding): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (mfcc_transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fusion): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pyds/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/miniconda3/envs/pyds/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "Training Metrics:\n",
      "Loss: 1.0945, Accuracy: 0.3864\n",
      "F1: 0.3700, Precision: 0.3895\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 1.0525, Accuracy: 0.2000\n",
      "F1: 0.1485, Precision: 0.4413\n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch0_acc0.2000_20250411_180520.pth\n",
      "\n",
      "Epoch 2/50\n",
      "Training Metrics:\n",
      "Loss: 1.0363, Accuracy: 0.4886\n",
      "F1: 0.4763, Precision: 0.5114\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.7499, Accuracy: 0.8667\n",
      "F1: 0.8087, Precision: 0.7641\n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch1_acc0.8667_20250411_180520.pth\n",
      "\n",
      "Epoch 3/50\n",
      "Training Metrics:\n",
      "Loss: 0.7906, Accuracy: 0.7102\n",
      "F1: 0.7002, Precision: 0.7137\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.5708, Accuracy: 0.7556\n",
      "F1: 0.7676, Precision: 0.9137\n",
      "\n",
      "Epoch 4/50\n",
      "Training Metrics:\n",
      "Loss: 0.6371, Accuracy: 0.7443\n",
      "F1: 0.7374, Precision: 0.7415\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.4518, Accuracy: 0.9556\n",
      "F1: 0.9556, Precision: 0.9556\n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch3_acc0.9556_20250411_180520.pth\n",
      "\n",
      "Epoch 5/50\n",
      "Training Metrics:\n",
      "Loss: 0.5703, Accuracy: 0.8182\n",
      "F1: 0.8135, Precision: 0.8140\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.4445, Accuracy: 0.9333\n",
      "F1: 0.9338, Precision: 0.9354\n",
      "\n",
      "Epoch 6/50\n",
      "Training Metrics:\n",
      "Loss: 0.5461, Accuracy: 0.8182\n",
      "F1: 0.8194, Precision: 0.8234\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3304, Accuracy: 0.8667\n",
      "F1: 0.8707, Precision: 0.8919\n",
      "\n",
      "Epoch 7/50\n",
      "Training Metrics:\n",
      "Loss: 0.5125, Accuracy: 0.8182\n",
      "F1: 0.8125, Precision: 0.8168\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3213, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 8/50\n",
      "Training Metrics:\n",
      "Loss: 0.4780, Accuracy: 0.8125\n",
      "F1: 0.8147, Precision: 0.8193\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3027, Accuracy: 0.8667\n",
      "F1: 0.8771, Precision: 0.9333\n",
      "\n",
      "Epoch 9/50\n",
      "Training Metrics:\n",
      "Loss: 0.5284, Accuracy: 0.8125\n",
      "F1: 0.7973, Precision: 0.8616\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3350, Accuracy: 0.8222\n",
      "F1: 0.8356, Precision: 0.9238\n",
      "\n",
      "Epoch 10/50\n",
      "Training Metrics:\n",
      "Loss: 0.4312, Accuracy: 0.7955\n",
      "F1: 0.7885, Precision: 0.8105\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2478, Accuracy: 0.9556\n",
      "F1: 0.9522, Precision: 0.9596\n",
      "\n",
      "Epoch 11/50\n",
      "Training Metrics:\n",
      "Loss: 0.3993, Accuracy: 0.8750\n",
      "F1: 0.8733, Precision: 0.8760\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3551, Accuracy: 0.8222\n",
      "F1: 0.8356, Precision: 0.9238\n",
      "\n",
      "Epoch 12/50\n",
      "Training Metrics:\n",
      "Loss: 0.3574, Accuracy: 0.8864\n",
      "F1: 0.8863, Precision: 0.8870\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2013, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch11_acc1.0000_20250411_180521.pth\n",
      "\n",
      "Epoch 13/50\n",
      "Training Metrics:\n",
      "Loss: 0.2890, Accuracy: 0.9318\n",
      "F1: 0.9305, Precision: 0.9340\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3172, Accuracy: 0.8444\n",
      "F1: 0.8553, Precision: 0.9037\n",
      "\n",
      "Epoch 14/50\n",
      "Training Metrics:\n",
      "Loss: 0.2886, Accuracy: 0.9148\n",
      "F1: 0.9150, Precision: 0.9164\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1892, Accuracy: 0.9556\n",
      "F1: 0.9561, Precision: 0.9587\n",
      "\n",
      "Epoch 15/50\n",
      "Training Metrics:\n",
      "Loss: 0.2669, Accuracy: 0.9205\n",
      "F1: 0.9198, Precision: 0.9232\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1542, Accuracy: 0.9778\n",
      "F1: 0.9783, Precision: 0.9810\n",
      "\n",
      "Epoch 16/50\n",
      "Training Metrics:\n",
      "Loss: 0.2030, Accuracy: 0.9602\n",
      "F1: 0.9600, Precision: 0.9602\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2669, Accuracy: 0.9111\n",
      "F1: 0.9151, Precision: 0.9309\n",
      "\n",
      "Epoch 17/50\n",
      "Training Metrics:\n",
      "Loss: 0.1970, Accuracy: 0.9489\n",
      "F1: 0.9488, Precision: 0.9510\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1561, Accuracy: 0.9778\n",
      "F1: 0.9777, Precision: 0.9788\n",
      "\n",
      "Epoch 18/50\n",
      "Training Metrics:\n",
      "Loss: 0.2014, Accuracy: 0.9432\n",
      "F1: 0.9432, Precision: 0.9432\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2990, Accuracy: 0.8667\n",
      "F1: 0.8753, Precision: 0.9116\n",
      "\n",
      "Epoch 19/50\n",
      "Training Metrics:\n",
      "Loss: 0.2140, Accuracy: 0.9545\n",
      "F1: 0.9545, Precision: 0.9549\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2009, Accuracy: 0.9778\n",
      "F1: 0.9777, Precision: 0.9788\n",
      "\n",
      "Epoch 20/50\n",
      "Training Metrics:\n",
      "Loss: 0.1530, Accuracy: 0.9659\n",
      "F1: 0.9659, Precision: 0.9674\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1423, Accuracy: 0.9778\n",
      "F1: 0.9777, Precision: 0.9788\n",
      "\n",
      "Epoch 21/50\n",
      "Training Metrics:\n",
      "Loss: 0.1261, Accuracy: 0.9773\n",
      "F1: 0.9773, Precision: 0.9776\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2680, Accuracy: 0.9111\n",
      "F1: 0.9151, Precision: 0.9309\n",
      "\n",
      "Epoch 22/50\n",
      "Training Metrics:\n",
      "Loss: 0.1142, Accuracy: 0.9830\n",
      "F1: 0.9829, Precision: 0.9838\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2124, Accuracy: 0.9556\n",
      "F1: 0.9561, Precision: 0.9587\n",
      "\n",
      "Epoch 23/50\n",
      "Training Metrics:\n",
      "Loss: 0.1412, Accuracy: 0.9545\n",
      "F1: 0.9545, Precision: 0.9560\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2235, Accuracy: 0.9778\n",
      "F1: 0.9777, Precision: 0.9788\n",
      "\n",
      "Epoch 24/50\n",
      "Training Metrics:\n",
      "Loss: 0.1534, Accuracy: 0.9545\n",
      "F1: 0.9545, Precision: 0.9559\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.4798, Accuracy: 0.8222\n",
      "F1: 0.8349, Precision: 0.8965\n",
      "\n",
      "Epoch 25/50\n",
      "Training Metrics:\n",
      "Loss: 0.1426, Accuracy: 0.9545\n",
      "F1: 0.9545, Precision: 0.9545\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2833, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 26/50\n",
      "Training Metrics:\n",
      "Loss: 0.0824, Accuracy: 0.9886\n",
      "F1: 0.9886, Precision: 0.9890\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3516, Accuracy: 0.8889\n",
      "F1: 0.8911, Precision: 0.9072\n",
      "\n",
      "Epoch 27/50\n",
      "Training Metrics:\n",
      "Loss: 0.0698, Accuracy: 0.9943\n",
      "F1: 0.9943, Precision: 0.9944\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3287, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 28/50\n",
      "Training Metrics:\n",
      "Loss: 0.0663, Accuracy: 0.9943\n",
      "F1: 0.9943, Precision: 0.9944\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2544, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 29/50\n",
      "Training Metrics:\n",
      "Loss: 0.0621, Accuracy: 0.9943\n",
      "F1: 0.9943, Precision: 0.9944\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2398, Accuracy: 0.9333\n",
      "F1: 0.9354, Precision: 0.9433\n",
      "\n",
      "Epoch 30/50\n",
      "Training Metrics:\n",
      "Loss: 0.0812, Accuracy: 0.9943\n",
      "F1: 0.9943, Precision: 0.9944\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2546, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 31/50\n",
      "Training Metrics:\n",
      "Loss: 0.0613, Accuracy: 0.9886\n",
      "F1: 0.9886, Precision: 0.9890\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2279, Accuracy: 0.9778\n",
      "F1: 0.9777, Precision: 0.9788\n",
      "\n",
      "Epoch 32/50\n",
      "Training Metrics:\n",
      "Loss: 0.0735, Accuracy: 0.9830\n",
      "F1: 0.9830, Precision: 0.9838\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.0525, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Epoch 33/50\n",
      "Training Metrics:\n",
      "Loss: 0.0695, Accuracy: 0.9886\n",
      "F1: 0.9886, Precision: 0.9890\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.0420, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Epoch 34/50\n",
      "Training Metrics:\n",
      "Loss: 0.0437, Accuracy: 0.9943\n",
      "F1: 0.9943, Precision: 0.9944\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1634, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 35/50\n",
      "Training Metrics:\n",
      "Loss: 0.0356, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.0824, Accuracy: 0.9778\n",
      "F1: 0.9777, Precision: 0.9788\n",
      "\n",
      "Epoch 36/50\n",
      "Training Metrics:\n",
      "Loss: 0.0400, Accuracy: 0.9943\n",
      "F1: 0.9943, Precision: 0.9944\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1399, Accuracy: 0.9778\n",
      "F1: 0.9770, Precision: 0.9788\n",
      "\n",
      "Epoch 37/50\n",
      "Training Metrics:\n",
      "Loss: 0.0307, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1559, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 38/50\n",
      "Training Metrics:\n",
      "Loss: 0.0496, Accuracy: 0.9943\n",
      "F1: 0.9943, Precision: 0.9944\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2733, Accuracy: 0.9333\n",
      "F1: 0.9338, Precision: 0.9354\n",
      "\n",
      "Epoch 39/50\n",
      "Training Metrics:\n",
      "Loss: 0.0427, Accuracy: 0.9886\n",
      "F1: 0.9886, Precision: 0.9886\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1311, Accuracy: 0.9333\n",
      "F1: 0.9350, Precision: 0.9385\n",
      "\n",
      "Epoch 40/50\n",
      "Training Metrics:\n",
      "Loss: 0.0845, Accuracy: 0.9716\n",
      "F1: 0.9716, Precision: 0.9724\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1141, Accuracy: 0.9556\n",
      "F1: 0.9548, Precision: 0.9567\n",
      "\n",
      "Epoch 41/50\n",
      "Training Metrics:\n",
      "Loss: 0.1936, Accuracy: 0.9489\n",
      "F1: 0.9481, Precision: 0.9541\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1147, Accuracy: 0.9778\n",
      "F1: 0.9777, Precision: 0.9788\n",
      "\n",
      "Epoch 42/50\n",
      "Training Metrics:\n",
      "Loss: 0.0671, Accuracy: 0.9773\n",
      "F1: 0.9774, Precision: 0.9779\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3512, Accuracy: 0.9111\n",
      "F1: 0.9106, Precision: 0.9198\n",
      "\n",
      "Epoch 43/50\n",
      "Training Metrics:\n",
      "Loss: 0.0761, Accuracy: 0.9773\n",
      "F1: 0.9771, Precision: 0.9782\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.0697, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Epoch 44/50\n",
      "Training Metrics:\n",
      "Loss: 0.0470, Accuracy: 0.9886\n",
      "F1: 0.9886, Precision: 0.9890\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1747, Accuracy: 0.9778\n",
      "F1: 0.9770, Precision: 0.9788\n",
      "\n",
      "Epoch 45/50\n",
      "Training Metrics:\n",
      "Loss: 0.0292, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2442, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 46/50\n",
      "Training Metrics:\n",
      "Loss: 0.0203, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2063, Accuracy: 0.9778\n",
      "F1: 0.9777, Precision: 0.9788\n",
      "\n",
      "Epoch 47/50\n",
      "Training Metrics:\n",
      "Loss: 0.0323, Accuracy: 0.9943\n",
      "F1: 0.9943, Precision: 0.9944\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1190, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 48/50\n",
      "Training Metrics:\n",
      "Loss: 0.0244, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.1624, Accuracy: 0.9778\n",
      "F1: 0.9770, Precision: 0.9788\n",
      "\n",
      "Epoch 49/50\n",
      "Training Metrics:\n",
      "Loss: 0.0467, Accuracy: 0.9886\n",
      "F1: 0.9886, Precision: 0.9887\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.2410, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n",
      "\n",
      "Epoch 50/50\n",
      "Training Metrics:\n",
      "Loss: 0.0162, Accuracy: 1.0000\n",
      "F1: 1.0000, Precision: 1.0000\n",
      "\n",
      "Validation Metrics:\n",
      "Loss: 0.3729, Accuracy: 0.9556\n",
      "F1: 0.9553, Precision: 0.9596\n"
     ]
    }
   ],
   "source": [
    "# 1. First, set up the data manager (as shown in previous code)\n",
    "data_manager = PlaySenseDataManager()\n",
    "\n",
    "# Add user data\n",
    "user1_data = UserData(\n",
    "    df=u1_df,\n",
    "    audio_splited=u1_audio_splited\n",
    ")\n",
    "data_manager.add_user_data(user_id=1, user_data=user1_data)\n",
    "\n",
    "user3_data = UserData(\n",
    "    df=u3_df,\n",
    "    audio_splited=u3_audio_splited\n",
    ")\n",
    "data_manager.add_user_data(user_id=3, user_data=user3_data)\n",
    "\n",
    "user4_data = UserData(\n",
    "    df=u4_df,\n",
    "    audio_splited=u4_audio_splited\n",
    ")\n",
    "data_manager.add_user_data(user_id=4, user_data=user4_data)\n",
    "\n",
    "# Merge data and create dataset\n",
    "data_manager.merge_all_users_data()\n",
    "data_manager.create_dataset()\n",
    "\n",
    "# 2. Initialize the model\n",
    "model = InertialMFCCTransformer(\n",
    "    inertial_dim=12,     # Derivative features dimension\n",
    "    mfcc_dim=13,         # MFCC features dimension\n",
    "    num_classes=data_manager.dataset.get_num_classes(),       # 3 users (1, 3, 4)\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# 3. Create trainer instance\n",
    "trainer = PlaySenseTrainer(\n",
    "    model=model,\n",
    "    data_manager=data_manager,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=50,\n",
    "    checkpoint_dir='checkpoints'\n",
    ")\n",
    "\n",
    "# 4. Start training\n",
    "trainer.train(batch_size=32, train_ratio=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
